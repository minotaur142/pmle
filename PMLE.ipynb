{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/statsmodels/compat/pandas.py:23: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import log_loss\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from utils import _add_constant,_hat_diag,_sigmoid_pred,_sigmoid_pred, _information_matrix, _predict, _predict_proba, _FLIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firth Class\n",
    "#Things left to do: confidence intervals, Cauchy, generalize marginal effects, multinomial\n",
    "class PMLE():\n",
    "    class Firth_Logit():\n",
    "        def __init__(self,num_iters=10000, alpha=0.01,add_int=True,lmbda=0.5,FLAC=False, FLIC=False):\n",
    "\n",
    "            self.alpha = alpha\n",
    "            self.num_iters = num_iters\n",
    "            self.add_int = add_int\n",
    "            self.lmbda=lmbda\n",
    "            self.FLAC = FLAC\n",
    "            self.FLIC=FLIC\n",
    "        \n",
    "        def firth_gd(self,X,y,weights):\n",
    "            y_pred = _sigmoid_pred(X=X,weights=weights)\n",
    "            H =_hat_diag(X,weights)\n",
    "            I = _information_matrix(X,weights)\n",
    "            U = np.matmul((y -y_pred + self.lmbda*H*(1 - 2*y_pred)),X)\n",
    "            weights += np.matmul(np.linalg.inv(I),U)*self.alpha\n",
    "            return weights\n",
    "        \n",
    "        def fit(self,X,y):\n",
    "            #add intercept if necessary\n",
    "            orig_X = X\n",
    "            if self.add_int==True:\n",
    "                X =_add_constant(X)\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "            #initialize weights\n",
    "            weights=np.ones(X.shape[1])\n",
    "            \n",
    "            \n",
    "            #Perform gradient descent\n",
    "            for i in range(self.num_iters):\n",
    "                weights = self.firth_gd(X,y,weights)\n",
    "            \n",
    "            if (self.FLAC==True)&(self.FLIC==True):\n",
    "                X,y,aug_sample_weights=_FLAC_aug(X,y,weights)\n",
    "                self.X = X\n",
    "                self.y = y\n",
    "                sklogit = LogisticRegression(solver='newton-cg',penalty='none',fit_intercept=False)\n",
    "                sklogit.fit(X,y,sample_weight=aug_sample_weights)\n",
    "                weights = sklogit.coef_[1:]\n",
    "                eta = np.dot(orig_X,weights)\n",
    "                target = y-eta\n",
    "                b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "                b0 = b0_model.params[0]\n",
    "                weights = np.insert(weights,0,b0)\n",
    "            \n",
    "            elif self.FLAC==True:\n",
    "                X,y,aug_sample_weights=_FLAC_aug(X,y,weights)\n",
    "                self.X = X\n",
    "                self.y = y\n",
    "                sklogit = LogisticRegression(solver='newton-cg',penalty='none',fit_intercept=False)\n",
    "                sklogit.fit(X,y,sample_weight=aug_sample_weights)\n",
    "                weights = sklogit.coef_\n",
    "                \n",
    "            \n",
    "            elif self.FLIC==True:\n",
    "                weights = weights[1:]\n",
    "                eta = np.dot(orig_X,weights)\n",
    "                target = y-eta\n",
    "                b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "                b0 = b0_model.params[0]\n",
    "                weights = np.insert(weights,0,b0)\n",
    "            \n",
    "            weights = pd.Series(weights.flatten(),index=self.X.columns)\n",
    "            self.weights = weights\n",
    "            \n",
    "            I = _information_matrix(X,weights)\n",
    "            hat_matrix_diag = _hat_diag(X,weights)\n",
    "            Hessian = -I\n",
    "            y_pred = _sigmoid_pred(X,weights)\n",
    "            \n",
    "            self.I = I\n",
    "            self.hat_matrix_diag = hat_matrix_diag\n",
    "            self.Hessian = Hessian\n",
    "            \n",
    "            \n",
    "            self.log_likelihood = (y*np.log(y_pred)+(1-y)*np.log(1-y_pred)).sum()+0.5*np.log(np.linalg.det(I))\n",
    "            \n",
    "            \n",
    "        def marginal_effects(self,values=None):\n",
    "                \n",
    "            def at_specific_values(self,values):\n",
    "                n_features = self.weights.shape[0]\n",
    "                if values.shape[0]==n_features-1:\n",
    "                    values = _add_constant(values)\n",
    "                \n",
    "                p = _sigmoid_pred(values,self.weights)\n",
    "                effs = np.ones(n_features)\n",
    "                for i in range(n_features):\n",
    "                    weights_copy = self.weights.copy()\n",
    "                    weights_copy[i]+=1\n",
    "                    new_p =_sigmoid_pred(values,weights_copy)\n",
    "                    effs[i] = new_p-p\n",
    "                return effs\n",
    "            \n",
    "            #at means\n",
    "            column_means = self.X.mean()\n",
    "            at_means = at_specific_values(weights=column_means)\n",
    "\n",
    "            #meaned\n",
    "            averaged_marg_effs = np.ones((self.X.shape[0],self.X.shape[1]))\n",
    "            for i in range(self.X.shape[0]):\n",
    "                row = self.X.iloc[i]\n",
    "                p = _sigmoid_pred(row,self.weights)\n",
    "                for j in range(self.weights.shape[0]):\n",
    "                    weights_copy = self.weights.copy()\n",
    "                    weights_copy[j]+=1\n",
    "                    new_p =_sigmoid_pred(row,weights_copy)\n",
    "                    eff = new_p-p\n",
    "                    averaged_marg_effs[i,j] = eff\n",
    "                ame = pd.DataFrame(averaged_marg_effs.mean(axis=0),index=self.X.columns, columns=['mean'])\n",
    "                ame['at_means'] = at_means\n",
    "            #user requested\n",
    "            if (type(values)==numpy.ndarray) | (type(values)==pandas.core.series.Series):\n",
    "                user_requested = at_specific_values(values)\n",
    "                ame['requested_values'] = user_requsted\n",
    "            return ame\n",
    "        \n",
    "         def predict(self,X):\n",
    "            if self.FLAC==True:\n",
    "                X = _FLAC_pred_aug(X)\n",
    "            if X.shape[1]==self.X.shape[1]-1:\n",
    "                X=_add_constant(X)\n",
    "            return _predict(X,self.weights)\n",
    "        \n",
    "        def predict_proba(self,X):\n",
    "            if self.FLAC==True:\n",
    "                X = _FLAC_pred_aug(X)\n",
    "            if X.shape[1]==self.X.shape[1]-1:\n",
    "                X=_add_constant(X)\n",
    "            return _predict_proba(X,self.weights)\n",
    "        \n",
    "    \n",
    "    class logF11():\n",
    "        def __init__(self,intercept=False):\n",
    "            self.intercept=False\n",
    "        \n",
    "        def data_augementation(self,df,y_var_name):\n",
    "            num_rows = 2*(df.shape[1]-1)\n",
    "            y_ind = df.columns.get_loc(y_var_name)\n",
    "\n",
    "            aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "\n",
    "            #augment y variable\n",
    "            aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "            y = aug[y_var_name]\n",
    "\n",
    "            #augment X variables\n",
    "            X = aug.drop(y_var_name,axis=1)\n",
    "            for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "                 X.iloc[rows:rows+2,ind]=1\n",
    "\n",
    "            #bring it all together\n",
    "            aug = pd.concat([y,X],axis=1)\n",
    "            f_df = df.append(aug)\n",
    "\n",
    "            #add offset\n",
    "            f_df['real_data']=1\n",
    "            f_df['real_data'][-aug.shape[0]:]=0\n",
    "            f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "\n",
    "            #reseparate\n",
    "            X = f_df.drop(y_var_name,axis=1)\n",
    "            y = f_df[y_var_name]\n",
    "            \n",
    "            self.X = X\n",
    "            self.y = y\n",
    "    \n",
    "            return X, y\n",
    "        \n",
    "        def fit(self,df,y_var_name):\n",
    "            X, y = self.data_augementation(df,y_var_name)\n",
    "            model = sm.Logit(y,X).fit()\n",
    "            weights = model.params\n",
    "            if self.intercept==True:\n",
    "                weights = _FLIC(X,weights)\n",
    "                X = _add_constant(X)\n",
    "            self.X = X\n",
    "            weights = pd.Series(weights,index=X.columns)\n",
    "            self.weights = weights\n",
    "        \n",
    "        def predict(self,X):\n",
    "            return _predict(X,self.weights)\n",
    "        \n",
    "        def predict_proba(self,X):\n",
    "            return _predict_proba(X,self.weights)\n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
