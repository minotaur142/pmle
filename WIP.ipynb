{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/statsmodels/compat/pandas.py:23: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import log_loss\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from utils import _add_constant,_hat_diag,_sigmoid_pred,_sigmoid_pred, _information_matrix, _predict, _predict_proba\n",
    "#import R package brglm\n",
    "base = importr('base')\n",
    "d = {'package.dependencies': 'package_dot_dependencies',\n",
    "     'package_dependencies': 'package_uscore_dependencies'}\n",
    "brglm = importr('brglm',robject_translations=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "features = [i.replace(' ', '_') for i in load_breast_cancer().feature_names.tolist()]\n",
    "\n",
    "breast_cancer_df = pd.DataFrame(load_breast_cancer().data,columns=features)\n",
    "target_df = pd.DataFrame(load_breast_cancer().target, columns=['y'])\n",
    "X = breast_cancer_df.iloc[:,:5]\n",
    "y = target_df\n",
    "\n",
    "df = pd.concat([target_df,breast_cancer_df],axis=1).iloc[:,:6]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small data set and rare event data set\n",
    "num_successes_for_rare = int(((((1-df.y.mean())*df.shape[0])/.95)-((1-df.y.mean())*df.shape[0]))//1)\n",
    "rare_inds = sorted(list(df[df.y==0].index) + random.sample(list(df[df.y==1].index),num_successes_for_rare))\n",
    "# small_inds = random.sample(sorted(list(df.index)),50)\n",
    "\n",
    "\n",
    "rare_df = df.iloc[rare_inds,:6]\n",
    "rare_X = rare_df.drop('y',axis=1)\n",
    "rare_y = rare_df['y']\n",
    "\n",
    "separation_df = pd.read_csv('separation_df.csv',index_col=0).iloc[:,]\n",
    "separation_X = separation_df.drop('y',axis=1)\n",
    "separation_y = separation_df['y']\n",
    "\n",
    "small_df = pd.read_csv('small_df.csv',index_col=0)\n",
    "small_X = small_df.drop('y',axis=1)\n",
    "small_y = small_df['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def formula_from_df(df,y_var_name):\n",
    "    features = list((df.drop(y_var_name,axis=1).columns))\n",
    "    formula = y_var_name + '~' + ' + '.join(features)\n",
    "    return formula\n",
    "\n",
    "def Sigmoid_Pred(X, weights):\n",
    "    z = np.dot(X,weights)\n",
    "    sig =  1/(1 + np.exp(-1*z))\n",
    "    sig = np.clip(sig,.000001,.999999)\n",
    "    return sig\n",
    "\n",
    "def hat_diag(X,weights):\n",
    "    Xt = X.transpose()\n",
    "    \n",
    "    #Get diagonal of error\n",
    "    y_pred = Sigmoid_Pred(X,weights)\n",
    "    W = np.diag(y_pred*(1-y_pred))\n",
    "   \n",
    "    #Calculate Fisher Information Matrix\n",
    "    I = np.linalg.multi_dot([Xt,W,X]) \n",
    "\n",
    "    #Get Diagonal of Hat Matrix\n",
    "    hat = np.linalg.multi_dot([W**0.5,X,np.linalg.inv(I),Xt,W**0.5])\n",
    "    hat_diag = np.diag(hat)\n",
    "    return hat_diag\n",
    "\n",
    "def marginal_effects(X,weights):\n",
    "    #at means\n",
    "    column_means = X.mean()\n",
    "    p = Sigmoid_Pred(column_means,weights)\n",
    "    at_means = np.ones(6)\n",
    "    for i in range(weights.shape[0]):\n",
    "        weights_copy = c.copy()\n",
    "        weights_copy[i]+=1\n",
    "        new_p =Sigmoid_Pred(means,weights_copy)\n",
    "        at_means[i] = new_p-p\n",
    "    \n",
    "    #meaned\n",
    "    averaged_marg_effs = np.ones((X.shape[0],X.shape[1]))\n",
    "    for i in range(X.shape[0]):\n",
    "        row = X.iloc[i]\n",
    "        p = Sigmoid_Pred(row,c)\n",
    "        for j in range(weights.shape[0]):\n",
    "            weights_copy = weights.copy()\n",
    "            weights_copy[j]+=1\n",
    "            new_p = 1/(1+np.exp(-np.dot(row,weights_copy)))\n",
    "            eff = new_p-p\n",
    "            averaged_marg_effs[i,j] = eff\n",
    "        ame = pd.DataFrame(averaged_marg_effs.mean(axis=0),index=X.columns, columns=['mean'])\n",
    "        ame['at_means'] = at_means\n",
    "    return ame\n",
    "\n",
    "def information_matrix(X,weights):\n",
    "    Xt = X.transpose()\n",
    "\n",
    "    #Get diagonal of error\n",
    "    y_pred = Sigmoid_Pred(X,weights)\n",
    "    W = np.diag(y_pred*(1-y_pred))\n",
    "\n",
    "    #Calculate Fisher Information Matrix\n",
    "    I = np.linalg.multi_dot([Xt,W,X])\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firth from scratch\n",
    "def firth_logit(X,y,num_iter=10000,alpha=0.01,add_int=True):\n",
    "    #add intercept if necessary\n",
    "    if add_int==True:\n",
    "        X = add_constant(X)\n",
    "    \n",
    "    #initialize weights\n",
    "    weights=np.ones(X.shape[1])\n",
    "    \n",
    "    #Perform gradient descent\n",
    "    for i in range(num_iter):\n",
    "        y_pred = Sigmoid_Pred(X,weights)\n",
    "        H = hat_diag(X,weights)\n",
    "        #Update weights\n",
    "        U = np.matmul((y -y_pred + H*(0.5 - y_pred)),X)\n",
    "        weights += np.matmul(np.linalg.inv(I),U)*alpha\n",
    "        if (i%1000==0):\n",
    "            print('log likelihood: ',(y*np.log(y_pred)+(1-y)*np.log(1-y_pred)).sum()+0.5*np.log(np.linalg.det(I)))\n",
    "    preds = Sigmoid_Pred(X,weights)\n",
    "    weights = pd.Series(weights,index=X.columns)\n",
    "    return preds, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firth_logit_r(df,y_var_name, formula, r_data=True, weights='none',all_coef_summary=False):\n",
    "    #convert data frame to R df\n",
    "    if r_data==False:\n",
    "        if type(weights) ==str:\n",
    "            with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "                df = ro.conversion.py2ri(df)\n",
    "        else:\n",
    "            with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "                df = ro.conversion.py2ri(df)\n",
    "                weights = ro.vectors.FloatVector(weights)\n",
    "    \n",
    "    #create firth logit model\n",
    "    if weights!='none':\n",
    "        model = brglm.brglm(formula, data = df, family='binomial',pl=True, weights=weights)\n",
    "    else:\n",
    "        model = brglm.brglm(formula, data = df, family='binomial',pl=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    summary = base.summary(model)\n",
    "    summary_dic = {}\n",
    "    for i in range(len(summary.names)):\n",
    "        try:\n",
    "            summary_dic[summary.names[i]]=pandas2ri.converter.ri2py(list(summary)[i])\n",
    "        except:\n",
    "            pass\n",
    "    columns = list(df.colnames)\n",
    "    columns[0]='Intercept'\n",
    "    if all_coef_summary==True:\n",
    "        coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns)\n",
    "    else:\n",
    "        coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns).Coef\n",
    "    \n",
    "    #get raw output and apply sigmoid\n",
    "    preds = ro.r.predict(model,df)\n",
    "    preds = 1/(1+np.exp(-np.array(preds)))       \n",
    "    return preds, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firth with hyperparameter\n",
    "def tuned_firth_logit(X,y,lmbda=0.5,num_iter=10000,alpha=0.01,add_int=True):\n",
    "    #add intercept if necessary\n",
    "    if add_int==True:\n",
    "        X = add_constant(X)\n",
    "    \n",
    "    #initialize weights\n",
    "    weights=np.ones(X.shape[1])\n",
    "    \n",
    "    #Perform gradient descent\n",
    "    for i in range(num_iter):\n",
    "        y_pred = Sigmoid_Pred(X,weights)\n",
    "        H = hat_diag(X,weights)\n",
    "        U = np.matmul((y -y_pred + lmbda*H*(1 - 2*y_pred)),X)\n",
    "        weights += np.matmul(np.linalg.inv(I),U)*alpha\n",
    "        if (i%1000==0):\n",
    "            print('log likelihood: ',(y*np.log(y_pred)+(1-y)*np.log(1-y_pred)).sum()+lmbda*np.log(np.linalg.det(I)))\n",
    "    return pd.Series(weights,index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLIC(X,y,num_iter=10000,lmbda=0.5):\n",
    "    #get firth logit coefs\n",
    "    coefs = tuned_firth_logit(X,y,lmbda=lmbda,num_iter=num_iter)\n",
    "    \n",
    "    #reestimate intercept\n",
    "    coefs.drop('const',inplace=True)\n",
    "    eta = np.dot(X,coefs)\n",
    "    target = y-eta\n",
    "    b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "    b0 = b0_model.params[0]\n",
    "    coefs = pd.Series(b0,index=['Int']).append(coefs)\n",
    "    \n",
    "    #get predictions\n",
    "    X=add_constant(X)\n",
    "    preds = Sigmoid_Pred(X.values,coefs.values)\n",
    "    return preds,coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLIC_brglm(df, y_var_name):\n",
    "    formula = formula_from_df(df,y_var_name)\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        df_r = ro.conversion.py2ri(df)\n",
    "    model = brglm.brglm(formula, data = df_r, family='binomial',pl=True)\n",
    "    summary = base.summary(model)\n",
    "    summary_dic = {}\n",
    "    for i in range(len(summary.names)):\n",
    "        try:\n",
    "            summary_dic[summary.names[i]]=pandas2ri.converter.ri2py(list(summary)[i])\n",
    "        except:\n",
    "            pass\n",
    "    columns = list(firth_small_r.colnames)\n",
    "    coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns).Coef[1:]\n",
    "    y_var_name = formula.split('~')[0].strip()\n",
    "    y = df[y_var_name].values\n",
    "    X = df.drop(y_var_name,axis=1)\n",
    "    eta = np.dot(X,coefs)\n",
    "    target = y-eta\n",
    "    b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "    b0 = pd.Series(b0_model.params[0],index=['Int'])\n",
    "    coefs = b0.append(coefs)\n",
    "    X=add_constant(X)\n",
    "    preds = Sigmoid_Pred(X.values,coefs.values)\n",
    "    return preds,coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLAC(X,y):\n",
    "    \n",
    "    init_rows = df.shape[0]\n",
    "    X = add_constant(X)\n",
    "    \n",
    "    \n",
    "    #Build Hat Matrix = (W**0.5)*X*((XtWX)^-1)*Xt*W**0.5\n",
    "    preds, weights = firth_logit_r(df=aug_df,\n",
    "                                 y_var_name=y_var_name,\n",
    "                                 formula=formula,\n",
    "                                 r_data=False,\n",
    "                                 weights=aug_sample_weights)\n",
    "    weights = model.params\n",
    "    H = hat_diag(X,weights)\n",
    "    \n",
    "    #Duplicate every row\n",
    "    double_df = df.append(df)\n",
    "    \n",
    "    #Create a new copy of the original data\n",
    "    pseudo_y_df = df\n",
    "    #Change y to 1-y\n",
    "    pseudo_y_df[y_var_name]=1-pseudo_y_df[y_var_name]\n",
    "    \n",
    "    #Append to doubled df\n",
    "    aug_df = double_df.append(pseudo_y_df)\n",
    "    \n",
    "    #Create dummy for real vs. duplicated/pseudo data\n",
    "    aug_df['real_data'] = 0\n",
    "    aug_df['real_data'][init_rows:]=1\n",
    "    \n",
    "    \n",
    "    #Create vector of weights = 1 for real data, hi/2 for augmentation data\n",
    "    aug_sample_weights = pd.Series(np.concatenate([np.ones(init_rows),H/2,H/2]))\n",
    "    \n",
    "    \n",
    "    #Get predictions and coefficients\n",
    "    logit = LogisticRegression(solver='newton-cg',penalty='none')\n",
    "    logit.fit(,y,sample_weights)\n",
    "    model = sm.Logit(y,).fit()\n",
    "    weights = model.params\n",
    "    \n",
    "    return preds, coefs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logF11(df,y_var_name,intercept=False):\n",
    "    '''Perform log-f(1,1) data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    num_rows = 2*(df.shape[1]-1)\n",
    "    y_ind = df.columns.get_loc(y_var_name)\n",
    "    \n",
    "    aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "    \n",
    "    #augment y variable\n",
    "    aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "    y = aug[y_var_name]\n",
    "    \n",
    "    #augment X variables\n",
    "    X = aug.drop(y_var_name,axis=1)\n",
    "    for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "         X.iloc[rows:rows+2,ind]=1\n",
    "    \n",
    "    #bring it all together\n",
    "    aug = pd.concat([y,X],axis=1)\n",
    "    f_df = df.append(aug)\n",
    "    \n",
    "    #add offset\n",
    "    f_df['real_data']=1\n",
    "    f_df['real_data'][-aug.shape[0]:]=0\n",
    "    \n",
    "    #reseparate\n",
    "    X = f_df.drop(y_var_name,axis=1)\n",
    "    y = f_df[y_var_name]\n",
    "    \n",
    "    #Calculate weights\n",
    "    weights = f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "    model = sm.Logit(y,X).fit()\n",
    "    coefs = model.params\n",
    "    if intercept==True:\n",
    "        eta = np.dot(X,coefs)\n",
    "        target = y-eta\n",
    "        b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "        b0 = pd.Series(b0_model.params[0],index=['Int'])\n",
    "        coefs = b0.append(coefs)\n",
    "        X=add_constant(X)\n",
    "\n",
    "    preds = Sigmoid_Pred(X.values,coefs)\n",
    "    return preds, coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate versions that may be worth keeping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-F(1,1) just augmentation\n",
    "def logF11_aug(df,y_var_name,R=False):\n",
    "    '''Perform log-f(1,1) data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    num_rows = 2*(df.shape[1]-1)\n",
    "    y_ind = df.columns.get_loc(y_var_name)\n",
    "    \n",
    "    aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "    \n",
    "    #augment y variable\n",
    "    aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "    y = aug[y_var_name]\n",
    "    \n",
    "    #augment X variables\n",
    "    X = aug.drop(y_var_name,axis=1)\n",
    "    for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "         X.iloc[rows:rows+2,ind]=1\n",
    "    \n",
    "    #bring it all together\n",
    "    aug = pd.concat([y,X],axis=1)\n",
    "    f_df = df.append(aug)\n",
    "    \n",
    "    #add offset\n",
    "    f_df['real_data']=1\n",
    "    f_df['real_data'][-aug.shape[0]:]=0\n",
    "    \n",
    "    #Calculate weights\n",
    "    weights = f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "    if R==True:\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    \n",
    "            f_df = ro.conversion.py2ri(f_df)\n",
    "            weights = ro.vectors.FloatVector(weights)\n",
    "    return f_df, weights      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAC just augmentation\n",
    "def FLAC_aug(df,y_var_name,R=False):\n",
    "    '''Perform FLAC data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    init_rows = df.shape[0]\n",
    "    X = add_constant(df.drop(y_var_name,axis=1))\n",
    "    y = df[y_var_name]\n",
    "    glm = GLM(y,X,family=sm.families.Binomial()).fit()\n",
    "#     hat = glm.get_hat_matrix_diag()\n",
    "    weights = glm.params\n",
    "    y_pred = glm.predict(test_X)\n",
    "    W = np.diag(y_pred*(1-y_pred))\n",
    "    test_XtWtest_X = np.linalg.multi_dot([test_X.transpose(),W,test_X])\n",
    "    I = np.linalg.inv(test_XtWtest_X)\n",
    "    hat = np.diag(np.linalg.multi_dot([W**0.5,test_X,I,test_X.transpose(),W**0.5]))\n",
    "\n",
    "    aug_df = pd.concat([df,df,df])\n",
    "    aug_df[y_var_name][init_rows*2:]=1-aug_df[y_var_name][init_rows*2:]\n",
    "    aug_df['pseudo_data'] = 0\n",
    "    aug_df['pseudo_data'][init_rows:]=1\n",
    "\n",
    "    aug_sample_weights = pd.Series(np.concatenate([np.ones(init_rows),hat/2,hat/2]))\n",
    "    if R==True:\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    \n",
    "            aug_df = ro.conversion.py2ri(aug_df)\n",
    "            aug_sample_weights = ro.vectors.FloatVector(aug_sample_weights)\n",
    "    return aug_df, aug_sample_weights\n",
    "    \n",
    "    #Now run this through brglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ro.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ro.vectors.FloatVector(np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning: invalid value encountered in true_divide\n",
      "  n_endog_mu = self._clean((1. - endog) / (1. - mu))\n"
     ]
    }
   ],
   "source": [
    "#created augmented datasets\n",
    "\n",
    "#log-f(1,1)\n",
    "# all_f, all_weights_f = logf11_aug(all_df,'y')\n",
    "small_f, small_weights_f = logf11_aug(small_df,'y')\n",
    "rare_f, rare_weights_f = logf11_aug(rare_df,'y')\n",
    "separation_f, separation_weights_f = logf11_aug(separation_df,'y')\n",
    "\n",
    "#FLAC\n",
    "# all_FLAC, all_weights_FLAC = FLAC_aug(all_df,'y')\n",
    "small_FLAC, small_weights_FLAC = FLAC_aug(small_df,'y')\n",
    "rare_FLAC, rare_weights_FLAC = FLAC_aug(rare_df,'y')\n",
    "separation_FLAC, separation_weights_FLAC = FLAC_aug(separation_df,'y')\n",
    "\n",
    "#created R augmented datasets\n",
    "\n",
    "#log-f(1,1)\n",
    "# all_f, all_weights_f = logf11_aug(all_df,'y')\n",
    "small_f_r, small_weights_f_r = logf11_aug(small_df,'y')\n",
    "rare_f_r, rare_weights_f_r = logf11_aug(rare_df,'y')\n",
    "separation_f_r, separation_weights_f_r = logf11_aug(separation_df,'y')\n",
    "\n",
    "#FLAC\n",
    "# all_FLAC, all_weights_FLAC = FLAC_aug(all_df,'y')\n",
    "small_FLAC_r, small_weights_FLAC_r = FLAC_aug(small_df,'y')\n",
    "rare_FLAC_r, rare_weights_FLAC_r = FLAC_aug(rare_df,'y')\n",
    "separation_FLAC_r, separation_weights_FLAC_r = FLAC_aug(separation_df,'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosted_firth(X,y,num_steps=500):\n",
    "    init_rows = X.shape[0]\n",
    "    aug_X = X.append(X)\n",
    "    aug_y = y.append(1-y)\n",
    "    eta0=np.full((1,,sm.Logit(aug_y,aug_X.const).fit().params[0])\n",
    "    weights = np.ones(aug_X.shape[1])\n",
    "    for i in range(num_steps):\n",
    "        hat_diag = hat(aug_X,weights)\n",
    "        y_pred = Sigmoid_Pred(aug_X,weights)\n",
    "        offset = (1+h/2)*(aug_y-y_pred*())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(df,y_var_name,X_train,y_train,X_test,y_test):\n",
    "    models = ['logit','l2','firth','FLIC','FLAC','log-f(1,1)','log-f(1,1)_with_int']\n",
    "\n",
    "    c_X_train = add_constant(X_train)\n",
    "    c_X_test = add_constant(X_test)\n",
    "    \n",
    "    base = sm.Logit(y_train,c_X_train).fit()\n",
    "    control_proba = base.predict(c_X_test)\n",
    "    control_preds = control_proba.round()\n",
    "    \n",
    "    l2 = LogisticRegression()\n",
    "    l2.fit(X_train,y_train)\n",
    "    l2_proba = l2.predict_proba(X_test)\n",
    "    l2_preds = l2.predict(X_test)\n",
    "    \n",
    "    firth_proba, firth_coefs = firth_logit(X_train,y_train)\n",
    "    firth_preds = firth_proba.round()\n",
    "    \n",
    "    FLIC_proba, FLIC_coefs = FLIC(X_train,y_train)\n",
    "    FLIC_preds = FLIC_proba.round()\n",
    "    \n",
    "    FLAC_proba, FLAC_coefs = FLAC_brglm(df,y_var_name)\n",
    "    FLAC_preds = FLAC_proba.round()\n",
    "    \n",
    "    logF11_proba, logF11_coefs = logF11(df,y_var_name)\n",
    "    logF11_preds = logF11_proba.round()\n",
    "    \n",
    "    logF11_int_proba, logF11_coefs = logF11(df,y_var_name,intercept=True)\n",
    "    logF11_int_preds = logF11_int_round()\n",
    "    \n",
    "    proba = [control_proba, l2_proba, firth_proba, FLIC_proba, FLAC_proba, logF11_proba, logF11_int_proba]\n",
    "    preds = [control_preds, l2_preds, firth_preds, FLIC_preds, FLAC_preds, logF11_preds, logF11_int_preds]\n",
    "    return proba, preds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLAC(X,y):\n",
    "    \n",
    "    init_rows = df.shape[0]\n",
    "    X = add_constant(X)\n",
    "    \n",
    "    \n",
    "    #Build Hat Matrix = (W**0.5)*X*((XtWX)^-1)*Xt*W**0.5\n",
    "    preds, weights = firth_logit_r(df=aug_df,\n",
    "                                 y_var_name=y_var_name,\n",
    "                                 formula=formula,\n",
    "                                 r_data=False,\n",
    "                                 weights=aug_sample_weights)\n",
    "    weights = model.params\n",
    "    H = hat_diag(X,weights)\n",
    "    \n",
    "    #Duplicate every row\n",
    "    double_df = df.append(df)\n",
    "    \n",
    "    #Create a new copy of the original data\n",
    "    pseudo_y_df = df\n",
    "    #Change y to 1-y\n",
    "    pseudo_y_df[y_var_name]=1-pseudo_y_df[y_var_name]\n",
    "    \n",
    "    #Append to doubled df\n",
    "    aug_df = double_df.append(pseudo_y_df)\n",
    "    \n",
    "    #Create dummy for real vs. duplicated/pseudo data\n",
    "    aug_df['real_data'] = 0\n",
    "    aug_df['real_data'][init_rows:]=1\n",
    "    \n",
    "    \n",
    "    #Create vector of weights = 1 for real data, hi/2 for augmentation data\n",
    "    aug_sample_weights = pd.Series(np.concatenate([np.ones(init_rows),H/2,H/2]))\n",
    "    \n",
    "    \n",
    "    #Get predictions and coefficients\n",
    "    logit = LogisticRegression(solver='newton-cg',penalty='none')\n",
    "    logit.fit(,y,sample_weights)\n",
    "    model = sm.Logit(y,).fit()\n",
    "    weights = model.params\n",
    "    \n",
    "    return preds, coefs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=False,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='none',\n",
       "                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklogit = LogisticRegression(solver='newton-cg',penalty='none',fit_intercept=False)\n",
    "sklogit.fit(small_X,small_y)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.Series(sklogit.coef_.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firth Class\n",
    "class PMLE():\n",
    "    class Firth_Logit():\n",
    "        def __init__(self,num_iters=10000, alpha=0.01,add_int=True,lmbda=0.5,FLAC=False, FLIC=False):\n",
    "\n",
    "            self.alpha = alpha\n",
    "            self.num_iters = num_iters\n",
    "            self.add_int = add_int\n",
    "            self.lmbda=lmbda\n",
    "            self.FLAC = FLAC\n",
    "            self.FLIC=FLIC\n",
    "        \n",
    "        def firth_gd(self,X,y,weights):\n",
    "            y_pred = _sigmoid_pred(X=X,weights=weights)\n",
    "            H =_hat_diag(X,weights)\n",
    "            I = _information_matrix(X,weights)\n",
    "            U = np.matmul((y -y_pred + self.lmbda*H*(1 - 2*y_pred)),X)\n",
    "            weights += np.matmul(np.linalg.inv(I),U)*self.alpha\n",
    "            return weights\n",
    "        \n",
    "        def fit(self,X,y):\n",
    "            #add intercept if necessary\n",
    "            orig_X = X\n",
    "            if self.add_int==True:\n",
    "                X =_add_constant(X)\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "            #initialize weights\n",
    "            weights=np.ones(X.shape[1])\n",
    "            \n",
    "            \n",
    "            #Perform gradient descent\n",
    "            for i in range(self.num_iters):\n",
    "                weights = self.firth_gd(X,y,weights)\n",
    "            \n",
    "            if self.FLAC==True:\n",
    "                init_rows = X.shape[0]\n",
    "                \n",
    "                H = _hat_diag(X,weights)\n",
    "                aug_sample_weights = pd.Series(np.concatenate([np.ones(init_rows),H/2,H/2]))\n",
    "                \n",
    "                X = X.append(X).append(X)\n",
    "                X['pseudo_data']=0\n",
    "                X['pseudo_data'][init_rows:]=1\n",
    "                self.X = X\n",
    "                y = y.append(y).append(1-y)\n",
    "                self.y = y\n",
    "                \n",
    "                sklogit = LogisticRegression(solver='newton-cg',penalty='none',fit_intercept=False)\n",
    "                sklogit.fit(X,y,sample_weight=aug_sample_weights)\n",
    "                weights = sklogit.coef_\n",
    "                \n",
    "            \n",
    "            if self.FLIC==True:\n",
    "                weights = weights[1:]\n",
    "                eta = np.dot(orig_X,weights)\n",
    "                target = y-eta\n",
    "                b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "                b0 = b0_model.params[0]\n",
    "                weights = np.insert(weights,0,b0)\n",
    "            \n",
    "            weights = pd.Series(weights.flatten(),index=self.X.columns)\n",
    "            self.weights = weights\n",
    "            \n",
    "            I = _information_matrix(X,weights)\n",
    "            hat_matrix_diag = _hat_diag(X,weights)\n",
    "            Hessian = -I\n",
    "            y_pred = _sigmoid_pred(X,weights)\n",
    "            \n",
    "            self.I = I\n",
    "            self.hat_matrix_diag = hat_matrix_diag\n",
    "            self.Hessian = Hessian\n",
    "            \n",
    "            \n",
    "            self.log_likelihood = (y*np.log(y_pred)+(1-y)*np.log(1-y_pred)).sum()+0.5*np.log(np.linalg.det(I))\n",
    "            \n",
    "            \n",
    "        def marginal_effects(self,values=None):\n",
    "                \n",
    "            def at_specific_values(self,values):\n",
    "                n_features = self.weights.shape[0]\n",
    "                if values.shape[0]==n_features-1:\n",
    "                    values = _add_constant(values)\n",
    "                \n",
    "                p = _sigmoid_pred(values,self.weights)\n",
    "                effs = np.ones(n_features)\n",
    "                for i in range(n_features):\n",
    "                    weights_copy = self.weights.copy()\n",
    "                    weights_copy[i]+=1\n",
    "                    new_p =_sigmoid_pred(values,weights_copy)\n",
    "                    effs[i] = new_p-p\n",
    "                return effs\n",
    "            #at means\n",
    "            column_means = self.X.mean()\n",
    "            at_means = at_specific_values(weights=column_means)\n",
    "\n",
    "            #meaned\n",
    "            averaged_marg_effs = np.ones((self.X.shape[0],self.X.shape[1]))\n",
    "            for i in range(self.X.shape[0]):\n",
    "                row = self.X.iloc[i]\n",
    "                p = _sigmoid_pred(row,self.weights)\n",
    "                for j in range(self.weights.shape[0]):\n",
    "                    weights_copy = self.weights.copy()\n",
    "                    weights_copy[j]+=1\n",
    "                    new_p =_sigmoid_pred(row,weights_copy)\n",
    "                    eff = new_p-p\n",
    "                    averaged_marg_effs[i,j] = eff\n",
    "                ame = pd.DataFrame(averaged_marg_effs.mean(axis=0),index=self.X.columns, columns=['mean'])\n",
    "                ame['at_means'] = at_means\n",
    "            #user requested\n",
    "            if (type(values)==numpy.ndarray) | (type(values)==pandas.core.series.Series):\n",
    "                user_requested = at_specific_values(values)\n",
    "                ame['requested_values'] = user_requsted\n",
    "            return ame\n",
    "        \n",
    "        def predict(self,X):\n",
    "            return _predict(X,self.weights)\n",
    "        \n",
    "        def predict_proba(self,X):\n",
    "            return _predict_proba(X,self.weights)\n",
    "    \n",
    "    class logF11():\n",
    "        def __init__(self,intercept=False):\n",
    "            self.intercept=False\n",
    "        \n",
    "        def data_augementation(self,df,y_var_name):\n",
    "            num_rows = 2*(df.shape[1]-1)\n",
    "            y_ind = df.columns.get_loc(y_var_name)\n",
    "\n",
    "            aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "\n",
    "            #augment y variable\n",
    "            aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "            y = aug[y_var_name]\n",
    "\n",
    "            #augment X variables\n",
    "            X = aug.drop(y_var_name,axis=1)\n",
    "            for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "                 X.iloc[rows:rows+2,ind]=1\n",
    "\n",
    "            #bring it all together\n",
    "            aug = pd.concat([y,X],axis=1)\n",
    "            f_df = df.append(aug)\n",
    "\n",
    "            #add offset\n",
    "            f_df['real_data']=1\n",
    "            f_df['real_data'][-aug.shape[0]:]=0\n",
    "            f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "\n",
    "            #reseparate\n",
    "            X = f_df.drop(y_var_name,axis=1)\n",
    "            y = f_df[y_var_name]\n",
    "            \n",
    "            self.X = X\n",
    "            self.y = y\n",
    "    \n",
    "            return X, y\n",
    "        \n",
    "        def fit(self,df,y_var_name):\n",
    "            X, y = self.data_augementation(df,y_var_name)\n",
    "            model = sm.Logit(y,X).fit()\n",
    "            weights = model.params\n",
    "            if self.intercept==True:\n",
    "                eta = np.dot(X,weights)\n",
    "                target = y-eta\n",
    "                b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "                b0 = b0_model.params[0]\n",
    "                weights = np.insert(weights,0,b0)\n",
    "                X = _add_constant(X)\n",
    "            self.X = X\n",
    "            weights = pd.Series(weights,index=X.columns)\n",
    "            self.weights = weights\n",
    "        \n",
    "        def predict(self,X):\n",
    "            return _predict(X,self.weights)\n",
    "        \n",
    "        def predict_proba(self,X):\n",
    "            return _predict_proba(X,self.weights)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLAC(df,y_var_name):\n",
    "    \n",
    "    init_rows = df.shape[0]\n",
    "    X = add_constant(df.drop(y_var_name,axis=1))\n",
    "    y = df[y_var_name]\n",
    "    \n",
    "    #Build Hat Matrix = (W**0.5)*X*((XtWX)^-1)*Xt*W**0.5\n",
    "    model = sm.Logit(y,X).fit()\n",
    "    weights = model.params\n",
    "    H = hat_diag(X,weights)\n",
    "    \n",
    "    #Duplicate every row\n",
    "    double_df = df.append(df)\n",
    "    \n",
    "    #Create a new copy of the original data\n",
    "    pseudo_y_df = df\n",
    "    #Change y to 1-y\n",
    "    pseudo_y_df[y_var_name]=1-pseudo_y_df[y_var_name]\n",
    "    #Append to doubled df\n",
    "    aug_df = double_df.append(pseudo_y_df)\n",
    "    \n",
    "    #Create dummy for real vs. duplicated/pseudo data\n",
    "    aug_df['real_data'] = 0\n",
    "    aug_df['real_data'][init_rows:]=1\n",
    "    \n",
    "    \n",
    "    #Create regression formula\n",
    "    formula = formula_from_df(aug_df,y_var_name)\n",
    "    \n",
    "    \n",
    "    #Create vector of weights = 1 for real data, hi/2 for augmentation data\n",
    "    aug_sample_weights = pd.Series(np.concatenate([np.ones(init_rows),H/2,H/2]))\n",
    "    \n",
    "    \n",
    "    #Get predictions and coefficients\n",
    "    preds, coefs = firth_logit_r(df=aug_df,\n",
    "                                 y_var_name=y_var_name,\n",
    "                                 formula=formula,\n",
    "                                 r_data=False,\n",
    "                                 weights=aug_sample_weights)\n",
    "    \n",
    "    return preds, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = PMLE.Firth_Logit(num_iters=500,FLAC=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "f.fit(small_X,small_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.57734877e+00, -3.89851841e+00,  2.98240122e-01,  2.63512212e-01,\n",
       "        3.23578875e-02,  1.00066483e+02,  1.56102313e+00])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -5.577349\n",
       "1     -3.898518\n",
       "2      0.298240\n",
       "3      0.263512\n",
       "4      0.032358\n",
       "5    100.066483\n",
       "6      1.561023\n",
       "dtype: float64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(w.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted(X,y,weights,sample_weights):\n",
    "    one = sample_weights * (y*_sigmoid_pred(X,weights)-1) * y\n",
    "    two = np.dot(X.transpose(),one)\n",
    "    return two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
    "\n",
    "z = expit(yz)\n",
    "z0 = sample_weight * (z - 1) * y\n",
    "\n",
    "grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z  = np.expit(y*np.dot(X,weights))\n",
    "z0 = sample_weights * (z-1) * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X,weights):\n",
    "    z = np.dot(X,weights)\n",
    "    hessian = ((X**2)*np.exp(z))/(1+np.exp(z))**2\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const              -13.939280\n",
       "mean_radius         -3.516043\n",
       "mean_texture         0.375923\n",
       "mean_perimeter       0.374394\n",
       "mean_area            0.018203\n",
       "mean_smoothness    115.487126\n",
       "dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firth.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.dot(int_small_X,firth.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (569,5) and (50,) not aligned: 5 (dim 1) != 50 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-1f30f272cb62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (569,5) and (50,) not aligned: 5 (dim 1) != 50 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(((int_small_X**2)),np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "int_small_X = add_constant(small_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firth.Hessian.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = np.dot(int_small_X.transpose(),np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "1-dimensional array given. Array must be two-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-b7d59668b0c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_small_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmulti_dot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mmulti_dot\u001b[0;34m(arrays)\u001b[0m\n\u001b[1;32m   2658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2660\u001b[0;31m     \u001b[0m_assert_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2662\u001b[0m     \u001b[0;31m# _multi_dot_three is much faster than _multi_dot_matrix_chain_order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_assert_2d\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             raise LinAlgError('%d-dimensional array given. Array must be '\n\u001b[0;32m--> 201\u001b[0;31m                     'two-dimensional' % a.ndim)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_stacked_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: 1-dimensional array given. Array must be two-dimensional"
     ]
    }
   ],
   "source": [
    "np.linalg.multi_dot([int_small_X.transpose(),np.exp(z),(1+np.exp(z))**-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to Series, length must be 6: given 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-407357226d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint_small_X\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[1;32m   1486\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[0;34m(left, right, axis)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \u001b[0;31m# GH17901\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mto_series\u001b[0;34m(right)\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1419\u001b[0;31m                     \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m                 )\n\u001b[1;32m   1421\u001b[0m             \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to coerce to Series, length must be 6: given 1"
     ]
    }
   ],
   "source": [
    "int_small_X*(np.exp(z)*(1+np.exp(z))**-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 6)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_small_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
