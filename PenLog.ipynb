{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/statsmodels/compat/pandas.py:23: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import log_loss\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.genmod.generalized_linear_model import GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "features = [i.replace(' ', '_') for i in load_breast_cancer().feature_names.tolist()]\n",
    "\n",
    "breast_cancer_df = pd.DataFrame(load_breast_cancer().data,columns=features)\n",
    "target_df = pd.DataFrame(load_breast_cancer().target, columns=['y'])\n",
    "X = breast_cancer_df\n",
    "y = target_df\n",
    "\n",
    "df = pd.concat([target_df,breast_cancer_df],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small, rare and perfectly-separated datasets\n",
    "\n",
    "all_df = df.iloc[:,:6]\n",
    "all_X = all_df.drop('y',axis=1)\n",
    "all_y = all_df['y']\n",
    "\n",
    "rare_df = df.iloc[rare_inds,:6]\n",
    "rare_X = rare_df.drop('y',axis=1)\n",
    "rare_y = rare_df['y']\n",
    "\n",
    "separation_df = pd.read_csv('separation_df.csv',index_col=0).iloc[:,:6]\n",
    "separation_X = separation_df.drop('y',axis=1)\n",
    "separation_y = separation_df['y']\n",
    "\n",
    "small_df = pd.read_csv('small_df.csv',index_col=0).iloc[:,:6]\n",
    "small_X = small_df.drop('y',axis=1)\n",
    "small_y = small_df['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def formula_from_df(df,y_var_name):\n",
    "    features = list((df.drop(y_var_name,axis=1).columns))\n",
    "    formula = y_var_name + '~' + ' + '.join(features)\n",
    "    return formula\n",
    "\n",
    "def Sigmoid_Pred(X, weights):\n",
    "    z = np.dot(X,weights)\n",
    "    sig =  (1 + np.exp(-1*z))**-1\n",
    "    sig = np.clip(sig,.000001,.999999)\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import R package brglm\n",
    "base = importr('base')\n",
    "d = {'package.dependencies': 'package_dot_dependencies',\n",
    "     'package_dependencies': 'package_uscore_dependencies'}\n",
    "brglm = importr('brglm',robject_translations=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firth_logit(df,y_var_name,  r_df=True, weights='none',all_coef_summary=False):\n",
    "    #convert data frame to R df\n",
    "    if r_df==False:\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            df = ro.conversion.py2ri(df)\n",
    "    #form regression formula\n",
    "    forumla = formula_from_df(df,y_var_name)\n",
    "    \n",
    "    #create firth logit model\n",
    "    if weights!='none':\n",
    "        model = brglm.brglm(formula, data = df, family='binomial',pl=True, weights=weights)\n",
    "    else:\n",
    "        model = brglm.brglm(formula, data = df, family='binomial',pl=True, weights=weights)\n",
    "    \n",
    "    #extract coefficients\n",
    "    summary = base.summary(model)\n",
    "    summary_dic = {}\n",
    "    for i in range(len(summary.names)):\n",
    "        try:\n",
    "            summary_dic[summary.names[i]]=pandas2ri.converter.ri2py(list(summary)[i])\n",
    "        except:\n",
    "            pass\n",
    "    columns = list(df.colnames)\n",
    "    columns[0]='Intercept'\n",
    "    if all_coef_summary==True:\n",
    "        coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns)\n",
    "    else:\n",
    "        coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns).Coef\n",
    "    \n",
    "    #get raw output and apply sigmoid\n",
    "    preds = ro.r.predict(model,df)\n",
    "    preds = Sigmoid_Pred(list(preds))          \n",
    "    return preds, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLIC(df, formula):\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        df_r = ro.conversion.py2ri(df)\n",
    "    model = brglm.brglm(formula, data = df_r, family='binomial',pl=True)\n",
    "    summary = base.summary(model)\n",
    "    summary_dic = {}\n",
    "    for i in range(len(summary.names)):\n",
    "        try:\n",
    "            summary_dic[summary.names[i]]=pandas2ri.converter.ri2py(list(summary)[i])\n",
    "        except:\n",
    "            pass\n",
    "    columns = list(firth_small_r.colnames)\n",
    "    coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns).Coef[1:]\n",
    "    y_var_name = formula.split('~')[0].strip()\n",
    "    y = df[y_var_name].values\n",
    "    X = add_constant(df.drop(y_var_name,axis=1))\n",
    "    eta = np.dot(X,coefs)\n",
    "    target = y-eta\n",
    "    b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "    b0 = b0_model.params[0]\n",
    "    coefs['intercept']=b0\n",
    "    preds = np.dot(X.values,coefs.values)\n",
    "    return preds,coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLAC(df,y_var_name):\n",
    "    \n",
    "    init_rows = df.shape[0]\n",
    "    total_rows = init_rows*3\n",
    "    X = add_constant(df.drop(y_var_name,axis=1))\n",
    "    y = df[y_var_name]\n",
    "    \n",
    "    #Build Hat Matrix = (W**0.5)*X*((XtWX)^-1)*Xt*W**0.5\n",
    "    model = sm.Logit(y,X).fit()\n",
    "    weights = model.params\n",
    "    y_pred = model.predict(X)\n",
    "    error = y_pred*(1-y_pred)\n",
    "    W = np.diag(error)\n",
    "    invXtWX = np.linalg.inv(np.linalg.multi_dot([X.transpose(),W,X]))\n",
    "    hat = np.diag(np.linalg.multi_dot([W**0.5,X,invXtWX,X.transpose(),W**0.5]))\n",
    "    \n",
    "    #Duplicate every row\n",
    "    double_df = df.append(df)\n",
    "    \n",
    "    #Create a new copy of the original data\n",
    "    pseudo_y_df = df\n",
    "    #Change y to 1-y\n",
    "    pseudo_y_df[y_var_name]=1-pseudo_y_df[y_var_name]\n",
    "    \n",
    "    #Append to doubled df\n",
    "    aug_df = double_df.append(pseudo_y_df)\n",
    "    \n",
    "    #Create dummy for real vs. duplicated/pseudo data\n",
    "    aug_df['real_data'] = 0\n",
    "    aug_df['real_data'][init_rows:]=1\n",
    "    print(aug_df.columns)\n",
    "    \n",
    "    #Create regression formula\n",
    "    formula = formula_from_df(aug_df,y_var_name)\n",
    "    print(formula)\n",
    "    \n",
    "    #Create vector of weights = 1 for real data, hi/2 for augmentation data\n",
    "    aug_sample_weights = pd.Series(np.concatenate([np.ones(init_rows),hat/2,hat/2]))\n",
    "    \n",
    "    #convert to R\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        aug_df_r = ro.conversion.py2ri(aug_df)\n",
    "        aug_sample_weights_r = ro.vectors.FloatVector(aug_sample_weights)\n",
    "    print(aug_df_r)\n",
    "    #Get predictions and coefficients\n",
    "    preds, coefs = get_r_firth_results(aug_df_r,formula,weights=aug_sample_weights_r)\n",
    "    preds = Sigmoid_Pred(X,coefs.Coef.values)\n",
    "    return preds, coefs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logF11(df,y_var_name,intercept=False):\n",
    "    '''Perform log-f(1,1) data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    num_rows = 2*(df.shape[1]-1)\n",
    "    y_ind = df.columns.get_loc(y_var_name)\n",
    "    \n",
    "    aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "    \n",
    "    #augment y variable\n",
    "    aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "    y = aug[y_var_name]\n",
    "    \n",
    "    #augment X variables\n",
    "    X = aug.drop(y_var_name,axis=1)\n",
    "    for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "         X.iloc[rows:rows+2,ind]=1\n",
    "    \n",
    "    #bring it all together\n",
    "    aug = pd.concat([y,X],axis=1)\n",
    "    f_df = df.append(aug)\n",
    "    \n",
    "    #add offset\n",
    "    f_df['real_data']=1\n",
    "    f_df['real_data'][-aug.shape[0]:]=0\n",
    "    \n",
    "    #reseparate\n",
    "    X = f_df.drop(y_var_name,axis=1)\n",
    "    y = f_df[y_var_name]\n",
    "    \n",
    "    #Calculate weights\n",
    "    weights = f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "    model = sm.Logit(y,X).fit()\n",
    "    coefs = logit.params\n",
    "    if intercept==True:\n",
    "        eta = np.dot(X,coefs)\n",
    "        target = y-eta\n",
    "        b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "        b0 = b0_model.params[0]\n",
    "        coefs['intercept']=b0\n",
    "        #how do I get this match above?\n",
    "    preds = Sigmoid_Pred(X.values,coefs)\n",
    "    return preds, coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate versions that may be worth keeping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-F(1,1) just augmentation\n",
    "def logF11_aug(df,y_var_name,R=False):\n",
    "    '''Perform log-f(1,1) data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    num_rows = 2*(df.shape[1]-1)\n",
    "    y_ind = df.columns.get_loc(y_var_name)\n",
    "    \n",
    "    aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "    \n",
    "    #augment y variable\n",
    "    aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "    y = aug[y_var_name]\n",
    "    \n",
    "    #augment X variables\n",
    "    X = aug.drop(y_var_name,axis=1)\n",
    "    for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "         X.iloc[rows:rows+2,ind]=1\n",
    "    \n",
    "    #bring it all together\n",
    "    aug = pd.concat([y,X],axis=1)\n",
    "    f_df = df.append(aug)\n",
    "    \n",
    "    #add offset\n",
    "    f_df['real_data']=1\n",
    "    f_df['real_data'][-aug.shape[0]:]=0\n",
    "    \n",
    "    #Calculate weights\n",
    "    weights = f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "    if R==True:\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    \n",
    "            f_df = ro.conversion.py2ri(f_df)\n",
    "            weights = ro.vectors.FloatVector(weights)\n",
    "    return f_df, weights      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAC just augmentation\n",
    "def FLAC_aug(df,y_var_name,R=False):\n",
    "    '''Perform FLAC data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    init_rows = df.shape[0]\n",
    "    X = add_constant(df.drop(y_var_name,axis=1))\n",
    "    y = df[y_var_name]\n",
    "    glm = GLM(y,X,family=sm.families.Binomial()).fit()\n",
    "#     hat = glm.get_hat_matrix_diag()\n",
    "    weights = glm.params\n",
    "    y_pred = glm.predict(test_X)\n",
    "    W = np.diag(y_pred*(1-y_pred))\n",
    "    test_XtWtest_X = np.linalg.multi_dot([test_X.transpose(),W,test_X])\n",
    "    I = np.linalg.inv(test_XtWtest_X)\n",
    "    hat = np.diag(np.linalg.multi_dot([W**0.5,test_X,I,test_X.transpose(),W**0.5]))\n",
    "\n",
    "    aug_df = pd.concat([df,df,df])\n",
    "    aug_df[y_var_name][init_rows*2:]=1-aug_df[y_var_name][init_rows*2:]\n",
    "    aug_df['pseudo_data'] = 0\n",
    "    aug_df['pseudo_data'][init_rows:]=1\n",
    "\n",
    "    aug_sample_weights = pd.Series(np.concatenate([np.ones(init_rows),hat/2,hat/2]))\n",
    "    if R==True:\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    \n",
    "            aug_df = ro.conversion.py2ri(aug_df)\n",
    "            aug_sample_weights = ro.vectors.FloatVector(aug_sample_weights)\n",
    "    return aug_df, aug_sample_weights\n",
    "    \n",
    "    #Now run this through brglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning: invalid value encountered in true_divide\n",
      "  n_endog_mu = self._clean((1. - endog) / (1. - mu))\n"
     ]
    }
   ],
   "source": [
    "#created augmented datasets\n",
    "\n",
    "#log-f(1,1)\n",
    "# all_f, all_weights_f = logf11_aug(all_df,'y')\n",
    "small_f, small_weights_f = logf11_aug(small_df,'y')\n",
    "rare_f, rare_weights_f = logf11_aug(rare_df,'y')\n",
    "separation_f, separation_weights_f = logf11_aug(separation_df,'y')\n",
    "\n",
    "#FLAC\n",
    "# all_FLAC, all_weights_FLAC = FLAC_aug(all_df,'y')\n",
    "small_FLAC, small_weights_FLAC = FLAC_aug(small_df,'y')\n",
    "rare_FLAC, rare_weights_FLAC = FLAC_aug(rare_df,'y')\n",
    "separation_FLAC, separation_weights_FLAC = FLAC_aug(separation_df,'y')\n",
    "\n",
    "#created R augmented datasets\n",
    "\n",
    "#log-f(1,1)\n",
    "# all_f, all_weights_f = logf11_aug(all_df,'y')\n",
    "small_f_r, small_weights_f_r = logf11_aug(small_df,'y')\n",
    "rare_f_r, rare_weights_f_r = logf11_aug(rare_df,'y')\n",
    "separation_f_r, separation_weights_f_r = logf11_aug(separation_df,'y')\n",
    "\n",
    "#FLAC\n",
    "# all_FLAC, all_weights_FLAC = FLAC_aug(all_df,'y')\n",
    "small_FLAC_r, small_weights_FLAC_r = FLAC_aug(small_df,'y')\n",
    "rare_FLAC_r, rare_weights_FLAC_r = FLAC_aug(rare_df,'y')\n",
    "separation_FLAC_r, separation_weights_FLAC_r = FLAC_aug(separation_df,'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Round One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877193\n",
      "0.11430574610890282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95        47\n",
      "           1       0.96      0.97      0.96        67\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "[[44  3]\n",
      " [ 2 65]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#baseline scores\n",
    "sklogit = LogisticRegression(penalty='none',solver='newton-cg')\n",
    "baseline = sklogit.fit(X_train,y_train)\n",
    "baseline_preds = baseline.predict(X_test)\n",
    "baseline_proba = baseline.predict_proba(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,baseline_preds))\n",
    "print(log_loss(y_test,baseline_proba))\n",
    "print(classification_report(y_test,baseline_preds))\n",
    "print(confusion_matrix(y_test,baseline_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9121265377855887\n",
      "0.8523725834797891\n",
      "0.8822495606326889\n",
      "0.7978910369068541\n",
      "1.386944549367799\n",
      "0.5237417849244933\n",
      "0.7192496484948084\n",
      "0.5509427897257464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89       212\n",
      "           1       1.00      0.86      0.92       357\n",
      "\n",
      "    accuracy                           0.91       569\n",
      "   macro avg       0.90      0.93      0.91       569\n",
      "weighted avg       0.93      0.91      0.91       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.83       212\n",
      "           1       1.00      0.77      0.87       357\n",
      "\n",
      "    accuracy                           0.85       569\n",
      "   macro avg       0.86      0.88      0.85       569\n",
      "weighted avg       0.89      0.85      0.85       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       212\n",
      "           1       1.00      0.81      0.90       357\n",
      "\n",
      "    accuracy                           0.88       569\n",
      "   macro avg       0.88      0.91      0.88       569\n",
      "weighted avg       0.91      0.88      0.88       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79       212\n",
      "           1       1.00      0.68      0.81       357\n",
      "\n",
      "    accuracy                           0.80       569\n",
      "   macro avg       0.82      0.84      0.80       569\n",
      "weighted avg       0.87      0.80      0.80       569\n",
      "\n",
      "[[212   0]\n",
      " [ 50 307]]\n",
      "[[211   1]\n",
      " [ 83 274]]\n",
      "[[212   0]\n",
      " [ 67 290]]\n",
      "[[212   0]\n",
      " [115 242]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#rare Comparison\n",
    "\n",
    "sklogit = LogisticRegression(penalty='none',solver='newton-cg')\n",
    "baseline = sklogit.fit(X,y)\n",
    "control = sklogit.fit(rare_X,rare_y)\n",
    "l2 = LogisticRegression()\n",
    "l2_model = l2.fit(rare_X,rare_y)\n",
    "logf_model = logF11(rare_df,'y')\n",
    "firth_rare_pred, firth_rare_coef = get_r_firth_results(firth_rare_r,firth_all_r,firth_formula)\n",
    "firth = Sigmoid_Pred(add_constant(breast_cancer_df),firth_rare_coef.Coef)\n",
    "\n",
    "logf_preds = logf_model.predict(all_X_f[all_X_f.real_data==1])\n",
    "logf_proba = logf_model.predict_proba(all_X_f[all_X_f.real_data==1])\n",
    "\n",
    "print(accuracy_score(y, control.predict(X)))\n",
    "print(accuracy_score(y, l2_model.predict(X)))\n",
    "print(accuracy_score(y,logf_preds))\n",
    "print(accuracy_score(target_df,firth.round()))\n",
    "\n",
    "\n",
    "print(log_loss(y, control.predict_proba(X)))\n",
    "print(log_loss(y, l2_model.predict_proba(X)))\n",
    "print(log_loss(y,logf_proba))\n",
    "print(log_loss(target_df,firth))\n",
    "\n",
    "print(classification_report(y, control.predict(X)))\n",
    "print(classification_report(y, l2_model.predict(X)))\n",
    "print(classification_report(y,logf_preds))\n",
    "print(classification_report(target_df,firth.round()))\n",
    "\n",
    "\n",
    "print(confusion_matrix(y, control.predict(X)))\n",
    "print(confusion_matrix(y, l2_model.predict(X)))\n",
    "print(confusion_matrix(y,logf_preds))\n",
    "print(confusion_matrix(target_df,firth.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033391915641477\n",
      "0.9314586994727593\n",
      "0.9525483304042179\n",
      "0.8840070298769771\n",
      "3.338568533003464\n",
      "2.3673523264772225\n",
      "0.12799869366056194\n",
      "0.36682604194498686\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.88       212\n",
      "           1       0.95      0.89      0.92       357\n",
      "\n",
      "    accuracy                           0.90       569\n",
      "   macro avg       0.89      0.91      0.90       569\n",
      "weighted avg       0.91      0.90      0.90       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       212\n",
      "           1       0.96      0.93      0.94       357\n",
      "\n",
      "    accuracy                           0.93       569\n",
      "   macro avg       0.92      0.93      0.93       569\n",
      "weighted avg       0.93      0.93      0.93       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       212\n",
      "           1       0.97      0.96      0.96       357\n",
      "\n",
      "    accuracy                           0.95       569\n",
      "   macro avg       0.95      0.95      0.95       569\n",
      "weighted avg       0.95      0.95      0.95       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       212\n",
      "           1       0.89      0.93      0.91       357\n",
      "\n",
      "    accuracy                           0.88       569\n",
      "   macro avg       0.88      0.87      0.87       569\n",
      "weighted avg       0.88      0.88      0.88       569\n",
      "\n",
      "[[196  16]\n",
      " [ 39 318]]\n",
      "[[198  14]\n",
      " [ 25 332]]\n",
      "[[201  11]\n",
      " [ 16 341]]\n",
      "[[171  41]\n",
      " [ 25 332]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#small Comparison\n",
    "sklogit = LogisticRegression(penalty='none',solver='newton-cg')\n",
    "control = sklogit.fit(small_X,small_y)\n",
    "l2 = LogisticRegression()\n",
    "l2_model = l2.fit(small_X,small_y)\n",
    "logf_model = logF11(small_df,'y')\n",
    "firth_small_pred, firth_small_coef = get_r_firth_results(firth_small_r,firth_all_r,firth_formula)\n",
    "firth = Sigmoid_Pred(add_constant(breast_cancer_df),firth_small_coef.Coef)\n",
    "\n",
    "logf_preds = logf_model.predict(all_X_f[all_X_f.real_data==1])\n",
    "logf_proba = logf_model.predict_proba(all_X_f[all_X_f.real_data==1])\n",
    "\n",
    "print(accuracy_score(y, control.predict(X)))\n",
    "print(accuracy_score(y, l2_model.predict(X)))\n",
    "print(accuracy_score(y,logf_preds))\n",
    "print(accuracy_score(target_df,firth.round()))\n",
    "\n",
    "\n",
    "print(log_loss(y, control.predict_proba(X)))\n",
    "print(log_loss(y, l2_model.predict_proba(X)))\n",
    "print(log_loss(y,logf_proba))\n",
    "print(log_loss(target_df,firth))\n",
    "\n",
    "print(classification_report(y, control.predict(X)))\n",
    "print(classification_report(y, l2_model.predict(X)))\n",
    "print(classification_report(y,logf_preds))\n",
    "print(classification_report(target_df,firth.round()))\n",
    "\n",
    "\n",
    "print(confusion_matrix(y, control.predict(X)))\n",
    "print(confusion_matrix(y, l2_model.predict(X)))\n",
    "print(confusion_matrix(y,logf_preds))\n",
    "print(confusion_matrix(target_df,firth.round()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
