{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/statsmodels/compat/pandas.py:23: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import log_loss\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from utils import _add_constant,_hat_diag,_sigmoid_pred,_sigmoid_pred, _information_matrix, _predict, _predict_proba\n",
    "#import R package brglm\n",
    "base = importr('base')\n",
    "d = {'package.dependencies': 'package_dot_dependencies',\n",
    "     'package_dependencies': 'package_uscore_dependencies'}\n",
    "brglm = importr('brglm',robject_translations=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "features = [i.replace(' ', '_') for i in load_breast_cancer().feature_names.tolist()]\n",
    "\n",
    "breast_cancer_df = pd.DataFrame(load_breast_cancer().data,columns=features)\n",
    "target_df = pd.DataFrame(load_breast_cancer().target, columns=['y'])\n",
    "X = breast_cancer_df.iloc[:,:5]\n",
    "y = target_df\n",
    "\n",
    "df = pd.concat([target_df,breast_cancer_df],axis=1).iloc[:,:6]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small data set and rare event data set\n",
    "num_successes_for_rare = int(((((1-df.y.mean())*df.shape[0])/.95)-((1-df.y.mean())*df.shape[0]))//1)\n",
    "rare_inds = sorted(list(df[df.y==0].index) + random.sample(list(df[df.y==1].index),num_successes_for_rare))\n",
    "# small_inds = random.sample(sorted(list(df.index)),50)\n",
    "\n",
    "\n",
    "rare_df = df.iloc[rare_inds,:6]\n",
    "rare_X = rare_df.drop('y',axis=1)\n",
    "rare_y = rare_df['y']\n",
    "\n",
    "separation_df = pd.read_csv('separation_df.csv',index_col=0).iloc[:,]\n",
    "separation_X = separation_df.drop('y',axis=1)\n",
    "separation_y = separation_df['y']\n",
    "\n",
    "small_df = pd.read_csv('small_df.csv',index_col=0)\n",
    "small_X = small_df.drop('y',axis=1)\n",
    "small_y = small_df['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def formula_from_df(df,y_var_name):\n",
    "    features = list((df.drop(y_var_name,axis=1).columns))\n",
    "    formula = y_var_name + '~' + ' + '.join(features)\n",
    "    return formula\n",
    "\n",
    "def Sigmoid_Pred(X, weights):\n",
    "    z = np.dot(X,weights)\n",
    "    sig =  1/(1 + np.exp(-1*z))\n",
    "    sig = np.clip(sig,.000001,.999999)\n",
    "    return sig\n",
    "\n",
    "def hat_diag(X,weights):\n",
    "    Xt = X.transpose()\n",
    "    \n",
    "    #Get diagonal of error\n",
    "    y_pred = Sigmoid_Pred(X,weights)\n",
    "    W = np.diag(y_pred*(1-y_pred))\n",
    "   \n",
    "    #Calculate Fisher Information Matrix\n",
    "    I = np.linalg.multi_dot([Xt,W,X]) \n",
    "\n",
    "    #Get Diagonal of Hat Matrix\n",
    "    hat = np.linalg.multi_dot([W**0.5,X,np.linalg.inv(I),Xt,W**0.5])\n",
    "    hat_diag = np.diag(hat)\n",
    "    return hat_diag\n",
    "\n",
    "def marginal_effects(X,weights):\n",
    "    #at means\n",
    "    column_means = X.mean()\n",
    "    p = Sigmoid_Pred(column_means,weights)\n",
    "    at_means = np.ones(6)\n",
    "    for i in range(weights.shape[0]):\n",
    "        weights_copy = c.copy()\n",
    "        weights_copy[i]+=1\n",
    "        new_p =Sigmoid_Pred(means,weights_copy)\n",
    "        at_means[i] = new_p-p\n",
    "    \n",
    "    #meaned\n",
    "    averaged_marg_effs = np.ones((X.shape[0],X.shape[1]))\n",
    "    for i in range(X.shape[0]):\n",
    "        row = X.iloc[i]\n",
    "        p = Sigmoid_Pred(row,c)\n",
    "        for j in range(weights.shape[0]):\n",
    "            weights_copy = weights.copy()\n",
    "            weights_copy[j]+=1\n",
    "            new_p = 1/(1+np.exp(-np.dot(row,weights_copy)))\n",
    "            eff = new_p-p\n",
    "            averaged_marg_effs[i,j] = eff\n",
    "        ame = pd.DataFrame(averaged_marg_effs.mean(axis=0),index=X.columns, columns=['mean'])\n",
    "        ame['at_means'] = at_means\n",
    "    return ame\n",
    "\n",
    "def information_matrix(X,weights):\n",
    "    Xt = X.transpose()\n",
    "\n",
    "    #Get diagonal of error\n",
    "    y_pred = Sigmoid_Pred(X,weights)\n",
    "    W = np.diag(y_pred*(1-y_pred))\n",
    "\n",
    "    #Calculate Fisher Information Matrix\n",
    "    I = np.linalg.multi_dot([Xt,W,X])\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = self.Sigmoid_Pred(X, weights)\n",
    "W = np.diag(y_pred*(1-y_pred))\n",
    "Xt = X.transpose()\n",
    "I = np.linalg.multi_dot([Xt,W,X]) \n",
    "hat = np.linalg.multi_dot([W**0.5,X,np.linalg.inv(I),Xt,W**0.5])\n",
    "hat_diag = np.diag(hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firth from scratch\n",
    "def firth_logit(X,y,num_iter=10000,alpha=0.01,add_int=True):\n",
    "    #add intercept if necessary\n",
    "    if add_int==True:\n",
    "        X = add_constant(X)\n",
    "    \n",
    "    #initialize weights\n",
    "    weights=np.ones(X.shape[1])\n",
    "    \n",
    "    #Perform gradient descent\n",
    "    for i in range(num_iter):\n",
    "        y_pred = Sigmoid_Pred(X,weights)\n",
    "        H = hat_diag(X,weights)\n",
    "        #Update weights\n",
    "        U = np.matmul((y -y_pred + H*(0.5 - y_pred)),X)\n",
    "        weights += np.matmul(np.linalg.inv(I),U)*alpha\n",
    "        if (i%1000==0):\n",
    "            print('log likelihood: ',(y*np.log(y_pred)+(1-y)*np.log(1-y_pred)).sum()+0.5*np.log(np.linalg.det(I)))\n",
    "    preds = Sigmoid_Pred(X,weights)\n",
    "    weights = pd.Series(weights,index=X.columns)\n",
    "    return preds, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firth_logit_r(df,y_var_name, formula, r_data=True, weights='none',all_coef_summary=False):\n",
    "    #convert data frame to R df\n",
    "    if r_data==False:\n",
    "        if type(weights) ==str:\n",
    "            with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "                df = ro.conversion.py2ri(df)\n",
    "        else:\n",
    "            with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "                df = ro.conversion.py2ri(df)\n",
    "                weights = ro.vectors.FloatVector(weights)\n",
    "    \n",
    "    #create firth logit model\n",
    "    if weights!='none':\n",
    "        model = brglm.brglm(formula, data = df, family='binomial',pl=True, weights=weights)\n",
    "    else:\n",
    "        model = brglm.brglm(formula, data = df, family='binomial',pl=True)\n",
    "    \n",
    "    #extract coefficients\n",
    "    summary = base.summary(model)\n",
    "    summary_dic = {}\n",
    "    for i in range(len(summary.names)):\n",
    "        try:\n",
    "            summary_dic[summary.names[i]]=pandas2ri.converter.ri2py(list(summary)[i])\n",
    "        except:\n",
    "            pass\n",
    "    columns = list(df.colnames)\n",
    "    columns[0]='Intercept'\n",
    "    if all_coef_summary==True:\n",
    "        coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns)\n",
    "    else:\n",
    "        coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns).Coef\n",
    "    \n",
    "    #get raw output and apply sigmoid\n",
    "    preds = ro.r.predict(model,df)\n",
    "    preds = 1/(1+np.exp(-np.array(preds)))       \n",
    "    return preds, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firth with hyperparameter\n",
    "def tuned_firth_logit(X,y,lmbda=0.5,num_iter=10000,alpha=0.01,add_int=True):\n",
    "    #add intercept if necessary\n",
    "    if add_int==True:\n",
    "        X = add_constant(X)\n",
    "    \n",
    "    #initialize weights\n",
    "    weights=np.ones(X.shape[1])\n",
    "    \n",
    "    #Perform gradient descent\n",
    "    for i in range(num_iter):\n",
    "        y_pred = Sigmoid_Pred(X,weights)\n",
    "        H = hat_diag(X,weights)\n",
    "        U = np.matmul((y -y_pred + lmbda*H*(1 - 2*y_pred)),X)\n",
    "        weights += np.matmul(np.linalg.inv(I),U)*alpha\n",
    "        if (i%1000==0):\n",
    "            print('log likelihood: ',(y*np.log(y_pred)+(1-y)*np.log(1-y_pred)).sum()+lmbda*np.log(np.linalg.det(I)))\n",
    "    return pd.Series(weights,index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLIC(X,y,num_iter=10000,lmbda=0.5):\n",
    "    #get firth logit coefs\n",
    "    coefs = tuned_firth_logit(X,y,lmbda=lmbda,num_iter=num_iter)\n",
    "    \n",
    "    #reestimate intercept\n",
    "    coefs.drop('const',inplace=True)\n",
    "    eta = np.dot(X,coefs)\n",
    "    target = y-eta\n",
    "    b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "    b0 = b0_model.params[0]\n",
    "    coefs = pd.Series(b0,index=['Int']).append(coefs)\n",
    "    \n",
    "    #get predictions\n",
    "    X=add_constant(X)\n",
    "    preds = Sigmoid_Pred(X.values,coefs.values)\n",
    "    return preds,coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLIC_brglm(df, y_var_name):\n",
    "    formula = formula_from_df(df,y_var_name)\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        df_r = ro.conversion.py2ri(df)\n",
    "    model = brglm.brglm(formula, data = df_r, family='binomial',pl=True)\n",
    "    summary = base.summary(model)\n",
    "    summary_dic = {}\n",
    "    for i in range(len(summary.names)):\n",
    "        try:\n",
    "            summary_dic[summary.names[i]]=pandas2ri.converter.ri2py(list(summary)[i])\n",
    "        except:\n",
    "            pass\n",
    "    columns = list(firth_small_r.colnames)\n",
    "    coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns).Coef[1:]\n",
    "    y_var_name = formula.split('~')[0].strip()\n",
    "    y = df[y_var_name].values\n",
    "    X = df.drop(y_var_name,axis=1)\n",
    "    eta = np.dot(X,coefs)\n",
    "    target = y-eta\n",
    "    b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "    b0 = pd.Series(b0_model.params[0],index=['Int'])\n",
    "    coefs = b0.append(coefs)\n",
    "    X=add_constant(X)\n",
    "    preds = Sigmoid_Pred(X.values,coefs.values)\n",
    "    return preds,coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLAC_brglm(df,y_var_name):\n",
    "    \n",
    "    init_rows = df.shape[0]\n",
    "    X = add_constant(df.drop(y_var_name,axis=1))\n",
    "    y = df[y_var_name]\n",
    "    \n",
    "    #Build Hat Matrix = (W**0.5)*X*((XtWX)^-1)*Xt*W**0.5\n",
    "    model = sm.Logit(y,X).fit()\n",
    "    weights = model.params\n",
    "    H = hat_diag(X,weights)\n",
    "    \n",
    "    #Duplicate every row\n",
    "    double_df = df.append(df)\n",
    "    \n",
    "    #Create a new copy of the original data\n",
    "    pseudo_y_df = df\n",
    "    #Change y to 1-y\n",
    "    pseudo_y_df[y_var_name]=1-pseudo_y_df[y_var_name]\n",
    "    \n",
    "    #Append to doubled df\n",
    "    aug_df = double_df.append(pseudo_y_df)\n",
    "    \n",
    "    #Create dummy for real vs. duplicated/pseudo data\n",
    "    aug_df['real_data'] = 0\n",
    "    aug_df['real_data'][init_rows:]=1\n",
    "    \n",
    "    \n",
    "    #Create regression formula\n",
    "    formula = formula_from_df(aug_df,y_var_name)\n",
    "    \n",
    "    \n",
    "    #Create vector of weights = 1 for real data, hi/2 for augmentation data\n",
    "    aug_sample_weights = pd.Series(np.concatenate([np.ones(init_rows),H/2,H/2]))\n",
    "    \n",
    "    \n",
    "    #Get predictions and coefficients\n",
    "    preds, coefs = firth_logit_r(df=aug_df,\n",
    "                                 y_var_name=y_var_name,\n",
    "                                 formula=formula,\n",
    "                                 r_data=False,\n",
    "                                 weights=aug_sample_weights)\n",
    "    \n",
    "    return preds, coefs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logF11(df,y_var_name,intercept=False):\n",
    "    '''Perform log-f(1,1) data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    num_rows = 2*(df.shape[1]-1)\n",
    "    y_ind = df.columns.get_loc(y_var_name)\n",
    "    \n",
    "    aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "    \n",
    "    #augment y variable\n",
    "    aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "    y = aug[y_var_name]\n",
    "    \n",
    "    #augment X variables\n",
    "    X = aug.drop(y_var_name,axis=1)\n",
    "    for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "         X.iloc[rows:rows+2,ind]=1\n",
    "    \n",
    "    #bring it all together\n",
    "    aug = pd.concat([y,X],axis=1)\n",
    "    f_df = df.append(aug)\n",
    "    \n",
    "    #add offset\n",
    "    f_df['real_data']=1\n",
    "    f_df['real_data'][-aug.shape[0]:]=0\n",
    "    \n",
    "    #reseparate\n",
    "    X = f_df.drop(y_var_name,axis=1)\n",
    "    y = f_df[y_var_name]\n",
    "    \n",
    "    #Calculate weights\n",
    "    weights = f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "    model = sm.Logit(y,X).fit()\n",
    "    coefs = model.params\n",
    "    if intercept==True:\n",
    "        eta = np.dot(X,coefs)\n",
    "        target = y-eta\n",
    "        b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "        b0 = pd.Series(b0_model.params[0],index=['Int'])\n",
    "        coefs = b0.append(coefs)\n",
    "        X=add_constant(X)\n",
    "\n",
    "    preds = Sigmoid_Pred(X.values,coefs)\n",
    "    return preds, coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate versions that may be worth keeping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log-F(1,1) just augmentation\n",
    "def logF11_aug(df,y_var_name,R=False):\n",
    "    '''Perform log-f(1,1) data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    num_rows = 2*(df.shape[1]-1)\n",
    "    y_ind = df.columns.get_loc(y_var_name)\n",
    "    \n",
    "    aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "    \n",
    "    #augment y variable\n",
    "    aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "    y = aug[y_var_name]\n",
    "    \n",
    "    #augment X variables\n",
    "    X = aug.drop(y_var_name,axis=1)\n",
    "    for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "         X.iloc[rows:rows+2,ind]=1\n",
    "    \n",
    "    #bring it all together\n",
    "    aug = pd.concat([y,X],axis=1)\n",
    "    f_df = df.append(aug)\n",
    "    \n",
    "    #add offset\n",
    "    f_df['real_data']=1\n",
    "    f_df['real_data'][-aug.shape[0]:]=0\n",
    "    \n",
    "    #Calculate weights\n",
    "    weights = f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "    if R==True:\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    \n",
    "            f_df = ro.conversion.py2ri(f_df)\n",
    "            weights = ro.vectors.FloatVector(weights)\n",
    "    return f_df, weights      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAC just augmentation\n",
    "def FLAC_aug(df,y_var_name,R=False):\n",
    "    '''Perform FLAC data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    init_rows = df.shape[0]\n",
    "    X = add_constant(df.drop(y_var_name,axis=1))\n",
    "    y = df[y_var_name]\n",
    "    glm = GLM(y,X,family=sm.families.Binomial()).fit()\n",
    "#     hat = glm.get_hat_matrix_diag()\n",
    "    weights = glm.params\n",
    "    y_pred = glm.predict(test_X)\n",
    "    W = np.diag(y_pred*(1-y_pred))\n",
    "    test_XtWtest_X = np.linalg.multi_dot([test_X.transpose(),W,test_X])\n",
    "    I = np.linalg.inv(test_XtWtest_X)\n",
    "    hat = np.diag(np.linalg.multi_dot([W**0.5,test_X,I,test_X.transpose(),W**0.5]))\n",
    "\n",
    "    aug_df = pd.concat([df,df,df])\n",
    "    aug_df[y_var_name][init_rows*2:]=1-aug_df[y_var_name][init_rows*2:]\n",
    "    aug_df['pseudo_data'] = 0\n",
    "    aug_df['pseudo_data'][init_rows:]=1\n",
    "\n",
    "    aug_sample_weights = pd.Series(np.concatenate([np.ones(init_rows),hat/2,hat/2]))\n",
    "    if R==True:\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    \n",
    "            aug_df = ro.conversion.py2ri(aug_df)\n",
    "            aug_sample_weights = ro.vectors.FloatVector(aug_sample_weights)\n",
    "    return aug_df, aug_sample_weights\n",
    "    \n",
    "    #Now run this through brglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:890: RuntimeWarning: invalid value encountered in true_divide\n",
      "  n_endog_mu = self._clean((1. - endog) / (1. - mu))\n"
     ]
    }
   ],
   "source": [
    "#created augmented datasets\n",
    "\n",
    "#log-f(1,1)\n",
    "# all_f, all_weights_f = logf11_aug(all_df,'y')\n",
    "small_f, small_weights_f = logf11_aug(small_df,'y')\n",
    "rare_f, rare_weights_f = logf11_aug(rare_df,'y')\n",
    "separation_f, separation_weights_f = logf11_aug(separation_df,'y')\n",
    "\n",
    "#FLAC\n",
    "# all_FLAC, all_weights_FLAC = FLAC_aug(all_df,'y')\n",
    "small_FLAC, small_weights_FLAC = FLAC_aug(small_df,'y')\n",
    "rare_FLAC, rare_weights_FLAC = FLAC_aug(rare_df,'y')\n",
    "separation_FLAC, separation_weights_FLAC = FLAC_aug(separation_df,'y')\n",
    "\n",
    "#created R augmented datasets\n",
    "\n",
    "#log-f(1,1)\n",
    "# all_f, all_weights_f = logf11_aug(all_df,'y')\n",
    "small_f_r, small_weights_f_r = logf11_aug(small_df,'y')\n",
    "rare_f_r, rare_weights_f_r = logf11_aug(rare_df,'y')\n",
    "separation_f_r, separation_weights_f_r = logf11_aug(separation_df,'y')\n",
    "\n",
    "#FLAC\n",
    "# all_FLAC, all_weights_FLAC = FLAC_aug(all_df,'y')\n",
    "small_FLAC_r, small_weights_FLAC_r = FLAC_aug(small_df,'y')\n",
    "rare_FLAC_r, rare_weights_FLAC_r = FLAC_aug(rare_df,'y')\n",
    "separation_FLAC_r, separation_weights_FLAC_r = FLAC_aug(separation_df,'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosted_firth(X,y,num_steps=500):\n",
    "    init_rows = X.shape[0]\n",
    "    aug_X = X.append(X)\n",
    "    aug_y = y.append(1-y)\n",
    "    eta0=np.full((1,,sm.Logit(aug_y,aug_X.const).fit().params[0])\n",
    "    weights = np.ones(aug_X.shape[1])\n",
    "    for i in range(num_steps):\n",
    "        hat_diag = hat(aug_X,weights)\n",
    "        y_pred = Sigmoid_Pred(aug_X,weights)\n",
    "        offset = (1+h/2)*(aug_y-y_pred*())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(df,y_var_name,X_train,y_train,X_test,y_test):\n",
    "    models = ['logit','l2','firth','FLIC','FLAC','log-f(1,1)','log-f(1,1)_with_int']\n",
    "\n",
    "    c_X_train = add_constant(X_train)\n",
    "    c_X_test = add_constant(X_test)\n",
    "    \n",
    "    base = sm.Logit(y_train,c_X_train).fit()\n",
    "    control_proba = base.predict(c_X_test)\n",
    "    control_preds = control_proba.round()\n",
    "    \n",
    "    l2 = LogisticRegression()\n",
    "    l2.fit(X_train,y_train)\n",
    "    l2_proba = l2.predict_proba(X_test)\n",
    "    l2_preds = l2.predict(X_test)\n",
    "    \n",
    "    firth_proba, firth_coefs = firth_logit(X_train,y_train)\n",
    "    firth_preds = firth_proba.round()\n",
    "    \n",
    "    FLIC_proba, FLIC_coefs = FLIC(X_train,y_train)\n",
    "    FLIC_preds = FLIC_proba.round()\n",
    "    \n",
    "    FLAC_proba, FLAC_coefs = FLAC_brglm(df,y_var_name)\n",
    "    FLAC_preds = FLAC_proba.round()\n",
    "    \n",
    "    logF11_proba, logF11_coefs = logF11(df,y_var_name)\n",
    "    logF11_preds = logF11_proba.round()\n",
    "    \n",
    "    logF11_int_proba, logF11_coefs = logF11(df,y_var_name,intercept=True)\n",
    "    logF11_int_preds = logF11_int_round()\n",
    "    \n",
    "    proba = [control_proba, l2_proba, firth_proba, FLIC_proba, FLAC_proba, logF11_proba, logF11_int_proba]\n",
    "    preds = [control_preds, l2_preds, firth_preds, FLIC_preds, FLAC_preds, logF11_preds, logF11_int_preds]\n",
    "    return proba, preds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firth Class\n",
    "class PMLE():\n",
    "    class Firth_Logit():\n",
    "        def __init__(self,num_iters=10000, alpha=0.01,add_int=True,lmbda=0.5,FLIC=False):\n",
    "\n",
    "            self.alpha = alpha\n",
    "            self.num_iters = num_iters\n",
    "            self.add_int = add_int\n",
    "            self.lmbda=lmbda\n",
    "            self.FLIC=FLIC\n",
    "        \n",
    "        def firth_gd(self,X,y,weights):\n",
    "            y_pred = _sigmoid_pred(X=X,weights=weights)\n",
    "            H =_hat_diag(X,weights)\n",
    "            I = _information_matrix(X,weights)\n",
    "            U = np.matmul((y -y_pred + self.lmbda*H*(1 - 2*y_pred)),X)\n",
    "            weights += np.matmul(np.linalg.inv(I),U)*self.alpha\n",
    "            return weights\n",
    "        \n",
    "        def fit(self,X,y):\n",
    "            #add intercept if necessary\n",
    "            orig_X = X\n",
    "            if self.add_int==True:\n",
    "                X =_add_constant(X)\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "            #initialize weights\n",
    "            weights=np.ones(X.shape[1])\n",
    "\n",
    "            #Perform gradient descent\n",
    "            for i in range(self.num_iters):\n",
    "                weights = self.firth_gd(X,y,weights)\n",
    "            \n",
    "            if self.FLIC==True:\n",
    "                weights = weights[1:]\n",
    "                eta = np.dot(orig_X,weights)\n",
    "                target = y-eta\n",
    "                b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "                b0 = b0_model.params[0]\n",
    "                weights = np.insert(weights,0,b0)\n",
    "            \n",
    "            weights = pd.Series(weights,index=X.columns)\n",
    "            self.weights = weights\n",
    "            \n",
    "            self.I = _information_matrix(X,weights)\n",
    "            self.hat_matrix_diag = _hat_diag(X,weights)\n",
    "            self.Hessian = -self.I\n",
    "            \n",
    "            \n",
    "        def marginal_effects(self):\n",
    "            #at means\n",
    "            column_means = self.X.mean()\n",
    "            p = _sigmoid_pred(column_means,self.weights)\n",
    "            at_means = np.ones(6)\n",
    "            for i in range(self.weights.shape[0]):\n",
    "                self.weights_copy = self.weights.copy()\n",
    "                self.weights_copy[i]+=1\n",
    "                new_p =_sigmoid_pred(column_means,self.weights_copy)\n",
    "                at_means[i] = new_p-p\n",
    "\n",
    "            #meaned\n",
    "            averaged_marg_effs = np.ones((self.X.shape[0],self.X.shape[1]))\n",
    "            for i in range(self.X.shape[0]):\n",
    "                row = self.X.iloc[i]\n",
    "                p = _sigmoid_pred(row,self.weights)\n",
    "                for j in range(self.weights.shape[0]):\n",
    "                    self.weights_copy = self.weights.copy()\n",
    "                    self.weights_copy[j]+=1\n",
    "                    new_p = 1/(1+np.exp(-np.dot(row,self.weights_copy)))\n",
    "                    eff = new_p-p\n",
    "                    averaged_marg_effs[i,j] = eff\n",
    "                ame = pd.DataFrame(averaged_marg_effs.mean(axis=0),index=self.X.columns, columns=['mean'])\n",
    "                ame['at_means'] = at_means\n",
    "            return ame\n",
    "        \n",
    "        def predict_proba(self,X):\n",
    "            if X.shape[1]==self.X.shape[1]-1:\n",
    "                X = _add_constant(X)\n",
    "            preds = _sigmoid_pred(X,self.weights)\n",
    "            return preds\n",
    "\n",
    "        def predict(self,X):\n",
    "            if X.shape[1]==self.X.shape[1]-1:\n",
    "                X = _add_constant(X)\n",
    "            preds = _sigmoid_pred(X,self.weights).round()\n",
    "            return preds\n",
    "    \n",
    "    class logF11():\n",
    "        def __init__(self,intercept=False):\n",
    "            self.intercept=False\n",
    "        \n",
    "        def data_augementation(self,df,y_var_name):\n",
    "            num_rows = 2*(df.shape[1]-1)\n",
    "            y_ind = df.columns.get_loc(y_var_name)\n",
    "\n",
    "            aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "\n",
    "            #augment y variable\n",
    "            aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "            y = aug[y_var_name]\n",
    "\n",
    "            #augment X variables\n",
    "            X = aug.drop(y_var_name,axis=1)\n",
    "            for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "                 X.iloc[rows:rows+2,ind]=1\n",
    "\n",
    "            #bring it all together\n",
    "            aug = pd.concat([y,X],axis=1)\n",
    "            f_df = df.append(aug)\n",
    "\n",
    "            #add offset\n",
    "            f_df['real_data']=1\n",
    "            f_df['real_data'][-aug.shape[0]:]=0\n",
    "            f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "\n",
    "            #reseparate\n",
    "            X = f_df.drop(y_var_name,axis=1)\n",
    "            y = f_df[y_var_name]\n",
    "            \n",
    "            self.X = X\n",
    "            self.y = y\n",
    "    \n",
    "            return X, y\n",
    "        \n",
    "        def fit(self,df,y_var_name):\n",
    "            X, y = self.data_augementation(df,y_var_name)\n",
    "            model = sm.Logit(y,X).fit()\n",
    "            weights = model.params\n",
    "            if self.intercept==True:\n",
    "                eta = np.dot(X,weights)\n",
    "                target = y-eta\n",
    "                b0_model = sm.OLS(target,np.ones(y.shape[0])).fit()\n",
    "                b0 = b0_model.params[0]\n",
    "                weights = np.insert(weights,0,b0)\n",
    "                X = _add_constant(X)\n",
    "            self.X = X\n",
    "            weights = pd.Series(weights,index=X.columns)\n",
    "            self.weights = weights\n",
    "        \n",
    "        def predict(self,X):\n",
    "            return _predict(X,self.weights)\n",
    "        \n",
    "        def predict_proba(self,X):\n",
    "            return _predict_proba(X,self.weights)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if i==num_iter:\n",
    "    print('log likelihood: ',(y*np.log(y_pred)+(1-y)*np.log(1-y_pred)).sum()+0.5*np.log(np.linalg.det(I)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out sample weights\n",
    "def _logistic_loss_and_grad(w, X, y, alpha, sample_weight=None):\n",
    "    \"\"\"Computes the logistic loss and gradient.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray of shape (n_features,) or (n_features + 1,)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Array of labels.\n",
    "    alpha : float\n",
    "        Regularization parameter. alpha is equal to 1 / C.\n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Array of weights that are assigned to individual samples.\n",
    "        If not provided, then each sample is given unit weight.\n",
    "    Returns\n",
    "    -------\n",
    "    out : float\n",
    "        Logistic loss.\n",
    "    grad : ndarray of shape (n_features,) or (n_features + 1,)\n",
    "        Logistic gradient.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    grad = np.empty_like(w)\n",
    "\n",
    "    w, c, yz = _intercept_dot(w, X, y)\n",
    "\n",
    "    if sample_weight is None:\n",
    "        sample_weight = np.ones(n_samples)\n",
    "\n",
    "    # Logistic loss is the negative of the log of the logistic function.\n",
    "    out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n",
    "\n",
    "    z = expit(yz)\n",
    "    z0 = sample_weight * (z - 1) * y\n",
    "\n",
    "    grad[:n_features] = safe_sparse_dot(X.T, z0) + alpha * w\n",
    "\n",
    "    # Case where we fit the intercept.\n",
    "    if grad.shape[0] > n_features:\n",
    "        grad[-1] = z0.sum()\n",
    "    return out, grad\n",
    "\n",
    "def safe_sparse_dot(a, b, dense_output=False):\n",
    "    \"\"\"Dot product that handle the sparse matrix case correctly\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : array or sparse matrix\n",
    "    b : array or sparse matrix\n",
    "    dense_output : boolean, (default=False)\n",
    "        When False, ``a`` and ``b`` both being sparse will yield sparse output.\n",
    "        When True, output will always be a dense array.\n",
    "    Returns\n",
    "    -------\n",
    "    dot_product : array or sparse matrix\n",
    "        sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.\n",
    "    \"\"\"\n",
    "    if a.ndim > 2 or b.ndim > 2:\n",
    "        if sparse.issparse(a):\n",
    "            # sparse is always 2D. Implies b is 3D+\n",
    "            # [i, j] @ [k, ..., l, m, n] -> [i, k, ..., l, n]\n",
    "            b_ = np.rollaxis(b, -2)\n",
    "            b_2d = b_.reshape((b.shape[-2], -1))\n",
    "            ret = a @ b_2d\n",
    "            ret = ret.reshape(a.shape[0], *b_.shape[1:])\n",
    "        elif sparse.issparse(b):\n",
    "            # sparse is always 2D. Implies a is 3D+\n",
    "            # [k, ..., l, m] @ [i, j] -> [k, ..., l, j]\n",
    "            a_2d = a.reshape(-1, a.shape[-1])\n",
    "            ret = a_2d @ b\n",
    "            ret = ret.reshape(*a.shape[:-1], b.shape[1])\n",
    "        else:\n",
    "            ret = np.dot(a, b)\n",
    "    else:\n",
    "        ret = a @ b\n",
    "\n",
    "    if (sparse.issparse(a) and sparse.issparse(b)\n",
    "            and dense_output and hasattr(ret, \"toarray\")):\n",
    "        return ret.toarray()\n",
    "    return ret\n",
    "\n",
    "def _intercept_dot(w, X, y):\n",
    "    \"\"\"Computes y * np.dot(X, w).\n",
    "    It takes into consideration if the intercept should be fit or not.\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : ndarray of shape (n_features,) or (n_features + 1,)\n",
    "        Coefficient vector.\n",
    "    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "        Training data.\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Array of labels.\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray of shape (n_features,)\n",
    "        Coefficient vector without the intercept weight (w[-1]) if the\n",
    "        intercept should be fit. Unchanged otherwise.\n",
    "    c : float\n",
    "        The intercept.\n",
    "    yz : float\n",
    "        y * np.dot(X, w).\n",
    "    \"\"\"\n",
    "    c = 0.\n",
    "    if w.size == X.shape[1] + 1:\n",
    "        c = w[-1]\n",
    "        w = w[:-1]\n",
    "\n",
    "    z = safe_sparse_dot(X, w) + c\n",
    "    yz = y * z\n",
    "    return w, c, yz\n",
    "\n",
    "def log_logistic(X, out=None):\n",
    "    \"\"\"Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.\n",
    "    This implementation is numerically stable because it splits positive and\n",
    "    negative values::\n",
    "        -log(1 + exp(-x_i))     if x_i > 0\n",
    "        x_i - log(1 + exp(x_i)) if x_i <= 0\n",
    "    For the ordinary logistic function, use ``scipy.special.expit``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (M, N) or (M, )\n",
    "        Argument to the logistic function\n",
    "    out : array-like, shape: (M, N) or (M, ), optional:\n",
    "        Preallocated output array.\n",
    "    Returns\n",
    "    -------\n",
    "    out : array, shape (M, N) or (M, )\n",
    "        Log of the logistic function evaluated at every point in x\n",
    "    Notes\n",
    "    -----\n",
    "    See the blog post describing this implementation:\n",
    "    http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/\n",
    "    \"\"\"\n",
    "    is_1d = X.ndim == 1\n",
    "    X = np.atleast_2d(X)\n",
    "    X = check_array(X, dtype=np.float64)\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    if out is None:\n",
    "        out = np.empty_like(X)\n",
    "\n",
    "    _log_logistic_sigmoid(n_samples, n_features, X, out)\n",
    "\n",
    "    if is_1d:\n",
    "        return np.squeeze(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3.191122e+03\n",
       "1    4.522077e+04\n",
       "2    6.358897e+04\n",
       "3    2.941959e+05\n",
       "4    1.999059e+06\n",
       "5    3.126354e+02\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(I.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const                2.749730\n",
       "mean_radius         28.778599\n",
       "mean_texture        52.892120\n",
       "mean_perimeter     187.054709\n",
       "mean_area          880.750305\n",
       "mean_smoothness      0.299444\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 1587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
