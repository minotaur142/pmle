{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/statsmodels/compat/pandas.py:23: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import log_loss\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "features = [i.replace(' ', '_') for i in load_breast_cancer().feature_names.tolist()]\n",
    "\n",
    "breast_cancer_df = pd.DataFrame(load_breast_cancer().data,columns=features)\n",
    "target_df = pd.DataFrame(load_breast_cancer().target, columns=['y'])\n",
    "X = breast_cancer_df\n",
    "y = target_df\n",
    "\n",
    "df = pd.concat([target_df,breast_cancer_df],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create small data set and rare event data set\n",
    "num_successes_for_rare = int(((((1-df.y.mean())*df.shape[0])/.95)-((1-df.y.mean())*df.shape[0]))//1)\n",
    "rare_inds = sorted(list(df[df.y==0].index) + random.sample(list(df[df.y==1].index),num_successes_for_rare))\n",
    "small_inds = random.sample(sorted(list(df.index)),50)\n",
    "rare_df = df.iloc[rare_inds,:]\n",
    "rare_X = rare_df.drop('y',axis=1)\n",
    "rare_y = rare_df['y']\n",
    "small_df = df.iloc[small_inds,:]\n",
    "small_X = small_df.drop('y',axis=1)\n",
    "small_y = small_df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DAP(df,y_var_name):\n",
    "    '''Perform log-f(1,1) data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    num_rows = 2*(df.shape[1]-1)\n",
    "    y_ind = df.columns.get_loc(y_var_name)\n",
    "    \n",
    "    aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "    \n",
    "    #augment y variable\n",
    "    aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "    y = aug[y_var_name]\n",
    "    \n",
    "    #augment X variables\n",
    "    X = aug.drop(y_var_name,axis=1)\n",
    "    for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "         X.iloc[rows:rows+2,ind]=1\n",
    "    \n",
    "    #bring it all together\n",
    "    aug = pd.concat([y,X],axis=1)\n",
    "    f_df = df.append(aug)\n",
    "    \n",
    "    #add offset\n",
    "    f_df['real_data']=1\n",
    "    f_df['real_data'][-aug.shape[0]:]=0\n",
    "    \n",
    "    #Calculate weights\n",
    "    weights = f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "    return f_df, weights      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "all_X_f,all_y_f, all_weights_f = DAP(df,'y')\n",
    "small_X_f,small_y_f, small_weights_f = DAP(small_df,'y')\n",
    "rare_X_f,rare_y_f, rare_weights_f = DAP(rare_df,'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert data for firth to R\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    \n",
    "    firth_all_r = ro.conversion.py2ri(df)\n",
    "    firth_small_r = ro.conversion.py2ri(small_df)\n",
    "    firth_rare_r = ro.conversion.py2ri(rare_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = importr('base')\n",
    "d = {'package.dependencies': 'package_dot_dependencies',\n",
    "     'package_dependencies': 'package_uscore_dependencies'}\n",
    "brglm = importr('brglm',robject_translations=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_firth_results(df,formula):\n",
    "    model = brglm.brglm(formula, data = df, family='binomial',pl=True)\n",
    "    summary = base.summary(model)\n",
    "    summary_dic = {}\n",
    "    for i in range(len(summary.names)):\n",
    "        try:\n",
    "            summary_dic[summary.names[i]]=pandas2ri.converter.ri2py(list(summary)[i])\n",
    "        except:\n",
    "            pass\n",
    "    columns = list(df.colnames)\n",
    "    columns[0]='Intercept'\n",
    "    coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns)\n",
    "    preds = ro.r.predict(model,firth_all_r)\n",
    "    preds = list(preds)             \n",
    "    return preds, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid_Pred(X, weights):\n",
    "    z = np.dot(X,weights)\n",
    "    sig =  (1 + np.exp(-1*z))**-1\n",
    "    sig = np.clip(sig,.000001,.999999)\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "firth_formula = 'y ~ ' + \" + \".join(features)\n",
    "flic_formula = firth_formula + ' - 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956140350877193\n",
      "0.11430574610890282\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95        47\n",
      "           1       0.96      0.97      0.96        67\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "[[44  3]\n",
      " [ 2 65]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#baseline scores\n",
    "sklogit = LogisticRegression(penalty='none',solver='newton-cg')\n",
    "baseline = sklogit.fit(X_train,y_train)\n",
    "baseline_preds = baseline.predict(X_test)\n",
    "baseline_proba = baseline.predict_proba(X_test)\n",
    "\n",
    "print(accuracy_score(y_test,baseline_preds))\n",
    "print(log_loss(y_test,baseline_proba))\n",
    "print(classification_report(y_test,baseline_preds))\n",
    "print(confusion_matrix(y_test,baseline_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9121265377855887\n",
      "0.8523725834797891\n",
      "0.8822495606326889\n",
      "0.7978910369068541\n",
      "1.386944549367799\n",
      "0.5237417849244933\n",
      "0.7192496484948084\n",
      "0.5509427897257464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89       212\n",
      "           1       1.00      0.86      0.92       357\n",
      "\n",
      "    accuracy                           0.91       569\n",
      "   macro avg       0.90      0.93      0.91       569\n",
      "weighted avg       0.93      0.91      0.91       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.83       212\n",
      "           1       1.00      0.77      0.87       357\n",
      "\n",
      "    accuracy                           0.85       569\n",
      "   macro avg       0.86      0.88      0.85       569\n",
      "weighted avg       0.89      0.85      0.85       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       212\n",
      "           1       1.00      0.81      0.90       357\n",
      "\n",
      "    accuracy                           0.88       569\n",
      "   macro avg       0.88      0.91      0.88       569\n",
      "weighted avg       0.91      0.88      0.88       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79       212\n",
      "           1       1.00      0.68      0.81       357\n",
      "\n",
      "    accuracy                           0.80       569\n",
      "   macro avg       0.82      0.84      0.80       569\n",
      "weighted avg       0.87      0.80      0.80       569\n",
      "\n",
      "[[212   0]\n",
      " [ 50 307]]\n",
      "[[211   1]\n",
      " [ 83 274]]\n",
      "[[212   0]\n",
      " [ 67 290]]\n",
      "[[212   0]\n",
      " [115 242]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#rare Comparison\n",
    "\n",
    "sklogit = LogisticRegression(penalty='none',solver='newton-cg')\n",
    "baseline = sklogit.fit(X,y)\n",
    "control = sklogit.fit(rare_X,rare_y)\n",
    "l2 = LogisticRegression()\n",
    "l2_model = l2.fit(rare_X,rare_y)\n",
    "logf_model = logF11(rare_df,'y')\n",
    "firth_rare_pred, firth_rare_coef = get_r_firth_results(firth_rare_r,firth_all_r,firth_formula)\n",
    "firth = Sigmoid_Pred(add_constant(breast_cancer_df),firth_rare_coef.Coef)\n",
    "\n",
    "logf_preds = logf_model.predict(all_X_f[all_X_f.real_data==1])\n",
    "logf_proba = logf_model.predict_proba(all_X_f[all_X_f.real_data==1])\n",
    "\n",
    "print(accuracy_score(y, control.predict(X)))\n",
    "print(accuracy_score(y, l2_model.predict(X)))\n",
    "print(accuracy_score(y,logf_preds))\n",
    "print(accuracy_score(target_df,firth.round()))\n",
    "\n",
    "\n",
    "print(log_loss(y, control.predict_proba(X)))\n",
    "print(log_loss(y, l2_model.predict_proba(X)))\n",
    "print(log_loss(y,logf_proba))\n",
    "print(log_loss(target_df,firth))\n",
    "\n",
    "print(classification_report(y, control.predict(X)))\n",
    "print(classification_report(y, l2_model.predict(X)))\n",
    "print(classification_report(y,logf_preds))\n",
    "print(classification_report(target_df,firth.round()))\n",
    "\n",
    "\n",
    "print(confusion_matrix(y, control.predict(X)))\n",
    "print(confusion_matrix(y, l2_model.predict(X)))\n",
    "print(confusion_matrix(y,logf_preds))\n",
    "print(confusion_matrix(target_df,firth.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033391915641477\n",
      "0.9314586994727593\n",
      "0.9525483304042179\n",
      "0.8840070298769771\n",
      "3.338568533003464\n",
      "2.3673523264772225\n",
      "0.12799869366056194\n",
      "0.36682604194498686\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.88       212\n",
      "           1       0.95      0.89      0.92       357\n",
      "\n",
      "    accuracy                           0.90       569\n",
      "   macro avg       0.89      0.91      0.90       569\n",
      "weighted avg       0.91      0.90      0.90       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       212\n",
      "           1       0.96      0.93      0.94       357\n",
      "\n",
      "    accuracy                           0.93       569\n",
      "   macro avg       0.92      0.93      0.93       569\n",
      "weighted avg       0.93      0.93      0.93       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       212\n",
      "           1       0.97      0.96      0.96       357\n",
      "\n",
      "    accuracy                           0.95       569\n",
      "   macro avg       0.95      0.95      0.95       569\n",
      "weighted avg       0.95      0.95      0.95       569\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       212\n",
      "           1       0.89      0.93      0.91       357\n",
      "\n",
      "    accuracy                           0.88       569\n",
      "   macro avg       0.88      0.87      0.87       569\n",
      "weighted avg       0.88      0.88      0.88       569\n",
      "\n",
      "[[196  16]\n",
      " [ 39 318]]\n",
      "[[198  14]\n",
      " [ 25 332]]\n",
      "[[201  11]\n",
      " [ 16 341]]\n",
      "[[171  41]\n",
      " [ 25 332]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2542: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#small Comparison\n",
    "sklogit = LogisticRegression(penalty='none',solver='newton-cg')\n",
    "control = sklogit.fit(small_X,small_y)\n",
    "l2 = LogisticRegression()\n",
    "l2_model = l2.fit(small_X,small_y)\n",
    "logf_model = logF11(small_df,'y')\n",
    "firth_small_pred, firth_small_coef = get_r_firth_results(firth_small_r,firth_all_r,firth_formula)\n",
    "firth = Sigmoid_Pred(add_constant(breast_cancer_df),firth_small_coef.Coef)\n",
    "\n",
    "logf_preds = logf_model.predict(all_X_f[all_X_f.real_data==1])\n",
    "logf_proba = logf_model.predict_proba(all_X_f[all_X_f.real_data==1])\n",
    "\n",
    "print(accuracy_score(y, control.predict(X)))\n",
    "print(accuracy_score(y, l2_model.predict(X)))\n",
    "print(accuracy_score(y,logf_preds))\n",
    "print(accuracy_score(target_df,firth.round()))\n",
    "\n",
    "\n",
    "print(log_loss(y, control.predict_proba(X)))\n",
    "print(log_loss(y, l2_model.predict_proba(X)))\n",
    "print(log_loss(y,logf_proba))\n",
    "print(log_loss(target_df,firth))\n",
    "\n",
    "print(classification_report(y, control.predict(X)))\n",
    "print(classification_report(y, l2_model.predict(X)))\n",
    "print(classification_report(y,logf_preds))\n",
    "print(classification_report(target_df,firth.round()))\n",
    "\n",
    "\n",
    "print(confusion_matrix(y, control.predict(X)))\n",
    "print(confusion_matrix(y, l2_model.predict(X)))\n",
    "print(confusion_matrix(y,logf_preds))\n",
    "print(confusion_matrix(target_df,firth.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r_firth_results(df, test_df, formula):\n",
    "    model = brglm.brglm(formula, data = df, family='binomial',pl=True)\n",
    "    summary = base.summary(model)\n",
    "    summary_dic = {}\n",
    "    for i in range(len(summary.names)):\n",
    "        try:\n",
    "            summary_dic[summary.names[i]]=pandas2ri.converter.ri2py(list(summary)[i])\n",
    "        except:\n",
    "            pass\n",
    "    columns = list(df.colnames)\n",
    "    columns[0]='Intercept'\n",
    "    coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns)\n",
    "    preds = ro.r.predict(model,firth_all_r)\n",
    "    preds = list(preds)             \n",
    "    return preds, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flic_step_1(df, test_df, formula):\n",
    "    model = brglm.brglm(flic_formula, data = firth_small_r, family='binomial',pl=True)\n",
    "    summary = base.summary(model)\n",
    "    summary_dic = {}\n",
    "    for i in range(len(summary.names)):\n",
    "        try:\n",
    "            summary_dic[summary.names[i]]=pandas2ri.converter.ri2py(list(summary)[i])\n",
    "        except:\n",
    "            pass\n",
    "    columns = list(firth_small_r.colnames)\n",
    "    coefs = pd.DataFrame(summary_dic['coefficients'],columns=(['Coef','SE','Z','P']),index=columns[1:])\n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test_X = add_constant(small_X)\n",
    "weights = coefs.iloc[:,0]\n",
    "weights['Int'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n",
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n",
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n",
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n",
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n",
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n",
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n",
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n",
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n",
      "4.973584440857107\n",
      "[0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999 0.999999\n",
      " 0.999999 0.999999]\n"
     ]
    }
   ],
   "source": [
    "for step in range(10000):\n",
    "    y_pred = Sigmoid_Pred(test_X, weights)\n",
    "    error = small_y - y_pred\n",
    "    weights['Int']+= (np.dot(test_X.const,error)*0.1)/small_X.shape[0]\n",
    "    if step % 1000 == 0:\n",
    "        print((-small_y * np.log(y_pred) - (1 - small_y) * np.log(1 - y_pred)).mean())\n",
    "        print(y_pred)\n",
    "        orint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999,\n",
       "       0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999,\n",
       "       0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999,\n",
       "       0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999,\n",
       "       0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999,\n",
       "       0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999,\n",
       "       0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999,\n",
       "       0.999999, 0.999999, 0.999999, 0.999999, 0.999999, 0.999999,\n",
       "       0.999999, 0.999999])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FLIC(X, y, weights, num_steps, alpha):\n",
    "    \n",
    "    X = add_constant(X)\n",
    "    weights['Intercept'] = 1\n",
    "    for step in range(num_steps):\n",
    "        y_pred = Sigmoid_Pred(X, weights)\n",
    "        error = y - y_pred\n",
    "        weights['Intercept']-= ((np.matmul(X.const,error))*alpha)/X.shape[0]\n",
    "        # Print log-likelihood every so often\n",
    "        if step % 1000 == 0:\n",
    "            print(weights['Intercept'])\n",
    "            print((-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred)).mean())\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00359999\n",
      "4.973584440857107\n",
      "4.603589990000034\n",
      "4.973584440857107\n",
      "8.2035799899997\n",
      "4.973584440857107\n",
      "11.803569989999364\n",
      "4.973584440857107\n",
      "15.403559989999028\n",
      "4.973584440857107\n",
      "19.003549990000174\n",
      "4.973584440857107\n",
      "22.603539990001615\n",
      "4.973584440857107\n",
      "26.203529990003055\n",
      "4.973584440857107\n",
      "29.803519990004496\n",
      "4.973584440857107\n",
      "33.40350999000455\n",
      "4.973584440857107\n",
      "37.00349999000244\n",
      "4.973584440857107\n",
      "40.60348999000033\n",
      "4.973584440857107\n",
      "44.203479989998215\n",
      "4.973584440857107\n",
      "47.8034699899961\n",
      "4.973584440857107\n",
      "51.40345998999399\n",
      "4.973584440857107\n",
      "55.00344998999188\n",
      "4.973584440857107\n",
      "58.603439989989766\n",
      "4.973584440857107\n",
      "62.203429989987654\n",
      "4.973584440857107\n",
      "65.80341998998554\n",
      "4.973584440857107\n",
      "69.40340998998343\n",
      "4.973584440857107\n",
      "73.00339998998132\n",
      "4.973584440857107\n",
      "76.6033899899792\n",
      "4.973584440857107\n",
      "80.2033799899771\n",
      "4.973584440857107\n",
      "83.80336998997498\n",
      "4.973584440857107\n",
      "87.40335998997287\n",
      "4.973584440857107\n",
      "91.00334998997076\n",
      "4.973584440857107\n",
      "94.60333998996865\n",
      "4.973584440857107\n",
      "98.20332998996653\n",
      "4.973584440857107\n",
      "101.80331998996442\n",
      "4.973584440857107\n",
      "105.40330998996231\n",
      "4.973584440857107\n",
      "109.0032999899602\n",
      "4.973584440857107\n",
      "112.60328998995809\n",
      "4.973584440857107\n",
      "116.20327998995597\n",
      "4.973584440857107\n",
      "119.80326998995386\n",
      "4.973584440857107\n",
      "123.40325998995175\n",
      "4.973584440857107\n",
      "127.00324998994964\n",
      "4.973584440857107\n",
      "130.60323998994753\n",
      "4.973584440857107\n",
      "134.2032299899454\n",
      "4.973584440857107\n",
      "137.8032199899433\n",
      "4.973584440857107\n",
      "141.4032099899412\n",
      "4.973584440857107\n",
      "145.00319998993908\n",
      "4.973584440857107\n",
      "148.60318998993696\n",
      "4.973584440857107\n",
      "152.20317998993485\n",
      "4.973584440857107\n",
      "155.80316998993274\n",
      "4.973584440857107\n",
      "159.40315998993063\n",
      "4.973584440857107\n",
      "163.00314998992852\n",
      "4.973584440857107\n",
      "166.6031399899264\n",
      "4.973584440857107\n",
      "170.2031299899243\n",
      "4.973584440857107\n",
      "173.80311998992218\n",
      "4.973584440857107\n",
      "177.40310998992007\n",
      "4.973584440857107\n",
      "181.00309998991796\n",
      "4.973584440857107\n",
      "184.60308998991584\n",
      "4.973584440857107\n",
      "188.20307998991373\n",
      "4.973584440857107\n",
      "191.80306998991162\n",
      "4.973584440857107\n",
      "195.4030599899095\n",
      "4.973584440857107\n",
      "199.0030499899074\n",
      "4.973584440857107\n",
      "202.60303998990528\n",
      "4.973584440857107\n",
      "206.20302998990317\n",
      "4.973584440857107\n",
      "209.80301998990106\n",
      "4.973584440857107\n",
      "213.40300998989895\n",
      "4.973584440857107\n",
      "217.00299998989684\n",
      "4.973584440857107\n",
      "220.60298998989472\n",
      "4.973584440857107\n",
      "224.2029799898926\n",
      "4.973584440857107\n",
      "227.8029699898905\n",
      "4.973584440857107\n",
      "231.4029599898884\n",
      "4.973584440857107\n",
      "235.00294998988628\n",
      "4.973584440857107\n",
      "238.60293998988416\n",
      "4.973584440857107\n",
      "242.20292998988205\n",
      "4.973584440857107\n",
      "245.80291998987994\n",
      "4.973584440857107\n",
      "249.40290998987783\n",
      "4.973584440857107\n",
      "253.00289998987571\n",
      "4.973584440857107\n",
      "256.60288998987363\n",
      "4.973584440857107\n",
      "260.2028799898715\n",
      "4.973584440857107\n",
      "263.8028699898694\n",
      "4.973584440857107\n",
      "267.4028599898673\n",
      "4.973584440857107\n",
      "271.0028499898652\n",
      "4.973584440857107\n",
      "274.60283998986307\n",
      "4.973584440857107\n",
      "278.20282998986096\n",
      "4.973584440857107\n",
      "281.80281998985885\n",
      "4.973584440857107\n",
      "285.40280998985673\n",
      "4.973584440857107\n",
      "289.0027999898546\n",
      "4.973584440857107\n",
      "292.6027899898525\n",
      "4.973584440857107\n",
      "296.2027799898504\n",
      "4.973584440857107\n",
      "299.8027699898483\n",
      "4.973584440857107\n",
      "303.4027599898462\n",
      "4.973584440857107\n",
      "307.00274998984406\n",
      "4.973584440857107\n",
      "310.60273998984195\n",
      "4.973584440857107\n",
      "314.20272998983984\n",
      "4.973584440857107\n",
      "317.8027199898377\n",
      "4.973584440857107\n",
      "321.4027099898356\n",
      "4.973584440857107\n",
      "325.0026999898335\n",
      "4.973584440857107\n",
      "328.6026899898314\n",
      "4.973584440857107\n",
      "332.2026799898293\n",
      "4.973584440857107\n",
      "335.80266998982717\n",
      "4.973584440857107\n",
      "339.40265998982505\n",
      "4.973584440857107\n",
      "343.00264998982294\n",
      "4.973584440857107\n",
      "346.60263998982083\n",
      "4.973584440857107\n",
      "350.2026299898187\n",
      "4.973584440857107\n",
      "353.8026199898166\n",
      "4.973584440857107\n",
      "357.4026099898145\n",
      "4.973584440857107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mean_radius                  1.917563\n",
       "mean_texture                -0.274275\n",
       "mean_perimeter              -0.296357\n",
       "mean_area                   -0.011428\n",
       "mean_smoothness            -14.196157\n",
       "mean_compactness             2.830915\n",
       "mean_concavity             -18.785380\n",
       "mean_concave_points         47.760462\n",
       "mean_symmetry               22.138644\n",
       "mean_fractal_dimension       6.489450\n",
       "radius_error               -18.513415\n",
       "texture_error               -1.378898\n",
       "perimeter_error              1.530872\n",
       "area_error                   0.044727\n",
       "smoothness_error          -122.537273\n",
       "compactness_error           18.670292\n",
       "concavity_error            -29.494201\n",
       "concave_points_error        18.803263\n",
       "symmetry_error              66.043840\n",
       "fractal_dimension_error    589.913428\n",
       "worst_radius                 1.891662\n",
       "worst_texture                0.224481\n",
       "worst_perimeter             -0.110844\n",
       "worst_area                  -0.006215\n",
       "worst_smoothness            32.321766\n",
       "worst_compactness           12.898039\n",
       "worst_concavity             -0.292633\n",
       "worst_concave_points       -24.083386\n",
       "worst_symmetry             -18.344622\n",
       "worst_fractal_dimension   -109.431242\n",
       "Intercept                  360.999000\n",
       "Name: Coef, dtype: float64"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLIC(small_X,small_y,coefs.Coef,100000,alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logF11(df,y_var_name):\n",
    "    '''Perform log-f(1,1) data augmentation\n",
    "       Returns augmented df and observation weights'''\n",
    "    \n",
    "    num_rows = 2*(df.shape[1]-1)\n",
    "    y_ind = df.columns.get_loc(y_var_name)\n",
    "    \n",
    "    aug = pd.DataFrame(0,columns=df.columns,index=(range(num_rows)))\n",
    "    \n",
    "    #augment y variable\n",
    "    aug.iloc[range(0,num_rows,2),y_ind]=1\n",
    "    y = aug[y_var_name]\n",
    "    \n",
    "    #augment X variables\n",
    "    X = aug.drop(y_var_name,axis=1)\n",
    "    for ind, rows in enumerate(range(0,X.shape[0],2)):\n",
    "         X.iloc[rows:rows+2,ind]=1\n",
    "    \n",
    "    #bring it all together\n",
    "    aug = pd.concat([y,X],axis=1)\n",
    "    f_df = df.append(aug)\n",
    "    \n",
    "    #add offset\n",
    "    f_df['real_data']=1\n",
    "    f_df['real_data'][-aug.shape[0]:]=0\n",
    "    \n",
    "    #reseparate\n",
    "    X = f_df.drop(y_var_name,axis=1)\n",
    "    y = f_df[y_var_name]\n",
    "    #Calculate weights\n",
    "    weights = f_df['real_data'].apply(lambda x: 0.5 if x == 0 else 1)\n",
    "    logit = LogisticRegression(penalty='none',solver='newton-cg',fit_intercept=False)\n",
    "    model = sklog.fit(X,y,sample_weight=weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exact logit\n",
    "#get all permutations of y values for given length\n",
    "def get_y_combos(length)\n",
    "    perms = []\n",
    "    step_1 = list(combinations_with_replacement([0,1],length))\n",
    "    for step in step_1:\n",
    "        perms += list(set(permutations(step,5)))\n",
    "    return perms\n",
    "#multiply each pair of X values by its corresponding y value\n",
    "ts = []\n",
    "for perm in perms:\n",
    "    t = [0,0]\n",
    "    for i in range(len(perm)):\n",
    "        t[0]+=perm[i]*ex_X.iloc[i][0].round()\n",
    "        t[1]+=perm[i]*ex_X.iloc[i][1].round()\n",
    "    ts.append(t)\n",
    "    \n",
    "#find a pair of ts where t1 = actual distributions t1\n",
    "actual_y_values_ind = perms.index((1,1,0,0,1))\n",
    "[t for t in ts if t[1] == ts[actual_y_values_ind][1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
